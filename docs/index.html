<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="icons/favicon-16x16.png">
    <link rel="icon" type="image/x-icon" href="icons/favicon.ico">
    <title>LLM Chess Leaderboard</title>
    <style>
        @font-face {
            font-family: 'Web IBM VGA 8x16';
            font-style: normal;
            font-weight: 400;
            src: url(web_ibm_vga_8x16.woff) format('woff');
        }

        @font-face {
            font-family: 'Chess';
            font-style: normal;
            font-weight: 400;
            /* src: url(segoe-w95-chess.woff) format('woff'); */
            src: url(DejavuSansMono.ttf) format('truetype');
        }

        body {
            background-color: #0000AA;
            color: #FFFFFF;
            font-family: "Web IBM VGA 8x16";
            text-align: center;
            margin: 0;
            font-size: 20px;
            overflow-y: scroll;
        }

        div.center {
            width: 740px;
            margin: auto;
            padding-bottom: 30px;
            padding-left: 20px;
            padding-right: 20px;
        }

        a {
            color: greenyellow;
            text-decoration: none; 
        }

        .gh {
            color: yellowgreen;
            text-decoration: none; 
            margin-right: 10px; 
        }

        h1 {
            margin-bottom: 20px;
        }

        p.intro {
            display: block;
            margin:auto;
            text-align: justify;
        }

        .button-container {
            display: flex;
            justify-content: center;
            margin-top: 50px;
            margin-bottom: 20px;
        }

        button {
            width: 160px;
            margin: 0 5px;
            padding: 5px;
            background-color: #C0C0C0;
            color: #000000;
            border: 1px solid #808080;
            font-family: "Web IBM VGA 8x16";
            font-size: 20px;
            position: relative;
            cursor: pointer;
        }

        button::after {
            content: '';
            position: absolute;
            top: 8px;
            left: 8px;
            width: 100%;
            height: 100%;
            background-color: #000000;
            z-index: -1;
        }

        button.selected {
            background-color: green;
            color: #FFFFFF;
            top: 1px;
            left: 1px;
        }

        button:hover {
            background-color: #808080;
            color: #FFFFFF;
            top: 2px;
            left: 2px;
        }

        .pane{
            padding-top: 20px;
        }

        pre.title {
            font-size: 16px;
            scale: 1.0;
            padding-top: 50px;
            color: greenyellow;
        }

        pre.snippet {
            font-family: "Web IBM VGA 8x16";
            text-align: left;
            color: greenyellow;
            overflow: auto;
        }

        pre.title-narrow {
            display: none;
        }

        .game {
            padding-bottom: 50px;
        }

        pre.board {
            font-family: 'Chess';
            font-size: 32px;
            width: 295px;
            height: 298px;
            background-color: black;
            text-align: justify;
            opacity: 0.85;
            line-height: 37px;
            margin: 0;
            padding-left: 5px;
            padding-top: 2px;
        }

        div.board::before {
            content: " ";
            display: block;
            position: absolute;
            top: 0;
            left: 0;
            bottom: 0;
            right: 0;
            background: linear-gradient(rgba(18, 16, 16, 0) 50%, rgba(0, 0, 0, 0.25) 50%), linear-gradient(90deg, rgba(255, 0, 0, 0.06), rgba(0, 255, 0, 0.02), rgba(0, 0, 255, 0.06));
            z-index: 2;
            background-size: 100% 2px, 3px 100%;
            pointer-events: none;
        }

        div.board {
            width: 300px;
            height: 300px;
            padding: 5px;
            margin-top: 20px;
            margin-bottom: 20px;
            margin-left: auto;
            margin-right: auto;
            position: relative;
        }

        .game-over {
            position: absolute;
            top: 0;
            left: 0;
            background-color: rgba(0, 0, 0, .9);
            margin: 5px;
            width: 300px;
            height: 300px;
            text-align: center;
            display: none;
        }

        @keyframes blink {
            0%, 100% {
                opacity: 1;
            }
            50% {
                opacity: 0;
            }
        }
        
        .game-over p {
            color: red;
            font-size: 40px;
            font-weight: bold;
            animation: blink 1s infinite;
        }

        .game-over span {
            text-align: left;
            display: inline-flex;
        }

        p, pre {
            color: lightgray;
        }

        .table-container {
            overflow-x: auto;
            padding: 0;
        }

        table {
            margin-bottom: 40px;
            min-width: 720px;
            width: 100%;
            border-collapse: collapse;
        }
        
        td, th {
            padding: 10px;
            font-weight: lighter;
            text-align: left;
        }

        th {
            cursor: pointer;
        }

        td {
            border-top: 1px solid;
            color: lightgray;
        }

        th:nth-child(even), td:nth-child(even) {
            background-color: black;
        }

        table tbody tr:nth-last-child(-n+3) td {
            color: greenyellow !important;
        }

        .descriptions {
            text-align: left;
            padding-top: 0;
        }
        
        .descriptions strong {
            font-weight: normal;
            color: white;
        }

        /* Responsive styles for mobile devices */
        @media (max-width: 768px) {
            body {
                font-size: 18px;
            }

            div.center {
                width: 90%;
            }

            h1 {
                font-size: 24px;
            }

            p.intro {
                width: 100%;
            }

            div.board {
                scale: 0.9;
            }

            button {
                width: 140px;
                font-size: 18px;
            }

            pre.title {
                display: none;
            }

            pre.title-narrow {
                display: block;
                font-size: 10px;
                padding-top: 20px;
                padding-bottom: 20px;
                color: white;
                max-width: 325px;
                overflow-x: hidden;
                margin: auto;
                line-height: 1.3;
                color: greenyellow;
            }

            td, th {
                padding: 8px;
            }

            .button-container {
                flex-direction: column;
                align-items: center;
            }

            .button-container button {
                margin: 5px 0;
            }
        }
    </style>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-4SS6VHS4QF"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4SS6VHS4QF');
    </script>
    <script>
            data = `
Player,total_games,player_wins,opponent_wins,draws,player_wins_percent,player_draws_percent,total_moves,player_wrong_actions,player_wrong_moves,player_avg_material,opponent_avg_material,material_diff_player_llm_minus_opponent,material_diff_player_minus_opponent_per_1000moves,wrong_actions_per_1000moves,wrong_moves_per_1000moves,average_moves,completion_tokens_black_per_move
o1-preview-2024-09-12,30,14,3,13,46.666666666666664,43.333333333333336,3744,4,10,11.512286324786325,1.5224358974358974,9.989850427350428,2.668229280809409,1.0683760683760686,2.6709401709401708,124.8,2660.071848290598
o1-mini-2024-09-12,30,9,6,15,30.0,50.0,4282,2,8,13.582438113031294,2.8169079869219993,10.765530126109295,2.5141359472464493,0.4670714619336758,1.8682858477347033,142.73333333333332,1221.1361513311538
anthropic.claude-v3-5-sonnet,30,0,8,22,0.0,73.33333333333333,5148,0,12,13.235431235431236,20.59770784770785,-7.362276612276613,-1.430123662058394,0.0,2.331002331002331,171.6,81.07478632478633
anthropic.claude-v3-haiku,40,0,40,0,0.0,0.0,1334,7,4,34.80209895052474,35.54422788605697,-0.7421289355322358,-0.556318542377988,5.247376311844079,2.9985007496251876,33.35,210.64017991004496
gemini-2.0-flash-exp,30,0,28,2,0.0,6.666666666666667,2576,0,81,17.729813664596275,17.31832298136646,0.41149068322981464,0.15974017206126345,0.0,31.444099378881987,85.86666666666666,168.15023291925465
gemini-2.0-flash-thinking-exp-1219,30,0,30,0,0.0,0.0,70,89,1,39.0,39.0,0.0,0.0,1271.4285714285713,14.285714285714286,2.3333333333333335,724.5428571428571
gemini-1.5-flash-001,30,0,20,10,0.0,33.33333333333333,2524,18,36,16.69136291600634,31.438985736925517,-14.747622820919176,-5.842956743628833,7.131537242472267,14.263074484944534,84.13333333333334,19.914817749603802
gemini-1.5-pro-preview-0409,30,0,27,3,0.0,10.0,2558,14,61,22.0852228303362,28.233776387802973,-6.148553557466773,-2.403656590096471,5.473025801407349,23.846755277560593,85.26666666666667,13.407740422204848
gemma-2-27b-it@q6_k_l,30,0,22,8,0.0,26.666666666666668,3268,8,52,18.94706242350061,20.79406364749082,-1.8470012239902083,-0.5651778531181788,2.4479804161566703,15.911872705018359,108.93333333333334,55.04436964504284
gemma-2-9b-it@q8_0,30,0,26,4,0.0,13.333333333333334,2075,45,31,23.387469879518072,26.799036144578313,-3.411566265060241,-1.6441283205109594,21.686746987951807,14.93975903614458,69.16666666666667,58.12433734939759
gpt-4-turbo-2024-04-09,30,0,2,28,0.0,93.33333333333333,5786,0,0,14.936571033529209,20.29001036985828,-5.353439336329071,-0.9252401203472298,0.0,0.0,192.86666666666667,6.032837884548911
gpt-4o-2024-05-13,30,0,5,25,0.0,83.33333333333334,5596,1,3,11.683345246604718,15.909042172980701,-4.225696926375983,-0.7551281140771949,0.17869907076483202,0.536097212294496,186.53333333333333,28.40654038598999
gpt-4o-2024-08-06,30,0,3,27,0.0,90.0,5861,0,0,14.68793721207985,17.37877495307968,-2.6908377409998305,-0.45910898157308144,0.0,0.0,195.36666666666667,6.579593925951203
gpt-4o-mini-2024-07-18,30,0,12,18,0.0,60.0,4481,13,24,17.146172729301494,21.37982593171167,-4.233653202410178,-0.9448009824615439,2.9011381388083017,5.355947333184558,149.36666666666667,108.21624637357732
deepseek-chat-v3,30,0,28,2,0.0,6.666666666666667,2193,9,64,22.48609211126311,22.114911080711355,0.3711810305517531,0.16925719587403243,4.1039671682626535,29.18376652986776,73.1,251.0592795257638
llama-3-70b-instruct-awq,30,0,15,15,0.0,50.0,4449,2,41,17.991458754776353,25.624185210159588,-7.632726455383235,-1.7156049573799135,0.4495392222971454,9.215554057091481,148.3,41.60890087660148
qwq-32b-preview@q4_k_m,30,0,30,0,0.0,0.0,239,44,17,38.8326359832636,38.90794979079498,-0.07531380753137995,-0.3151205336041002,184.10041841004184,71.12970711297072,7.966666666666667,2908.0
qwen2.5-14b-instruct@q8_0,30,0,30,0,0.0,0.0,398,29,59,38.19095477386934,37.79899497487437,0.391959798994975,0.9848236155652639,72.8643216080402,148.24120603015075,13.266666666666667,150.63065326633165
sky-t1-32b-preview@q4_0,30,0,30,0,0.0,0.0,415,9,59,38.40240963855422,38.62409638554217,-0.22168674698794888,-0.5341849325010816,21.686746987951807,142.1686746987952,13.833333333333334,1216.1590361445783
meta-llama-3.1-8b-instruct@fp16,30,0,30,0,0.0,0.0,830,19,61,34.69638554216868,35.29397590361446,-0.597590361445782,-0.7199883872840749,22.89156626506024,73.49397590361447,27.666666666666668,71.86024096385542
phi-4@q8,30,0,30,0,0.0,0.0,232,44,46,39.0,39.0,0.0,0.0,189.65517241379308,198.27586206896552,7.733333333333333,333.5431034482759
gpt-35-turbo-0125,30,0,30,0,0.0,0.0,86,90,0,39.0,39.0,0.0,0.0,1046.5116279069769,0.0,2.8666666666666667,82.02325581395348
Stockfish chess engine (as Black),1000,1000,0,0,100.000,0.000,32368,0,0,38.21,21.01,17.21,0.53164,0.00,0.00,32.37,-
Random Player (as White),1000,105,0,895,10.5,89.5,190073,0,0,10.56,11.08,-0.52,-0.00274,0.00,0.00,190.07,-
Random Player (as Black),1000,0,105,895,0.000,89.5,190073,0,0,11.08,10.56,0.52,0.00274,0.00,0.00,190.07,-
            `

        let currentSortOrder = {}; // To track the current sort order for each column
        const defaultSortColumnIndex = 1; // Sort by "Draws %" by default
        const SPECIAL_ROWS = {
            STOCKFISH: "Stockfish chess engine (as Black)",
            RANDOM_WHITE: "Random Player (as White)",
            RANDOM_BLACK: "Random Player (as Black)"
        };

        function showPane(paneId) {
            document.getElementById('leaderboard').style.display = paneId === 'leaderboard' ? 'block' : 'none';
            document.getElementById('how-it-works').style.display = paneId === 'how-it-works' ? 'block' : 'none';
            document.getElementById('considerations').style.display = paneId === 'considerations' ? 'block' : 'none';
            document.querySelectorAll('.button-container button').forEach(button => {
                button.classList.remove('selected');
            });
            document.querySelector(`.button-container button[onclick="showPane('${paneId}')"]`).classList.add('selected');

            // Simulate a page view event in Google Analytics
            gtag('event', 'page_view', {
                'page_title': document.title + ' - ' + paneId,
                'page_path': '/' + paneId
            });
        }

        // !!!! Copy Paste aggregate results CSV here
        function fillTableWithCSV() {
            const lines = data.trim().split('\n');
            const header = lines[0].split(',');
            const rows = lines.slice(1).map(row => row.split(','));

            const csvIndices = {
                player: 0,
                player_wins_percent: 5,
                player_draws_percent: 6,
                wrong_actions_per_1000moves: 14,
                wrong_moves_per_1000moves: 15,
                completion_tokens_black_per_move: 17
            };

            // Separate the rows that should always be at the bottom
            const bottomRows = [];
            const otherRows = rows.filter(columns => {
                const player = columns[csvIndices.player];
                if (player === SPECIAL_ROWS.STOCKFISH ||
                    player === SPECIAL_ROWS.RANDOM_WHITE ||
                    player === SPECIAL_ROWS.RANDOM_BLACK) {
                    bottomRows.push(columns);
                    return false;
                }
                return true;
            });

            // Sort the remaining rows by "Wins", "Draws", and then "Mistakes" in descending order
            otherRows.sort((a, b) => {
                const winsA = parseFloat(a[csvIndices.player_wins_percent]);
                const winsB = parseFloat(b[csvIndices.player_wins_percent]);
                const drawsA = parseFloat(a[csvIndices.player_draws_percent]);
                const drawsB = parseFloat(b[csvIndices.player_draws_percent]);
                const mistakesA = parseFloat(a[csvIndices.wrong_actions_per_1000moves]) + parseFloat(a[csvIndices.wrong_moves_per_1000moves]);
                const mistakesB = parseFloat(b[csvIndices.wrong_actions_per_1000moves]) + parseFloat(b[csvIndices.wrong_moves_per_1000moves]);

                // Hierarchical sorting: Wins DESC, Draws DESC, Mistakes ASC
                return winsB - winsA || drawsB - drawsA || mistakesA - mistakesB;
            });

            const tbody = document.querySelector('#leaderboard tbody');
            [...otherRows, ...bottomRows].forEach(columns => {
                const player = columns[csvIndices.player];
                const player_wins_percent = parseFloat(columns[csvIndices.player_wins_percent]);
                const player_draws_percent = parseFloat(columns[csvIndices.player_draws_percent]);
                const wrong_actions_per_1000moves = parseFloat(columns[csvIndices.wrong_actions_per_1000moves]);
                const wrong_moves_per_1000moves = parseFloat(columns[csvIndices.wrong_moves_per_1000moves]);
                const mistakes = wrong_actions_per_1000moves + wrong_moves_per_1000moves;
                const tokens = parseFloat(columns[csvIndices.completion_tokens_black_per_move]);

                const tr = document.createElement('tr');
                tr.innerHTML = `
                    <td>${player}</td> <!-- Player first -->
                    <td>${player_wins_percent.toFixed(2)}%</td>
                    <td>${player_draws_percent.toFixed(2)}%</td>
                    <td>${mistakes.toFixed(2)}</td>
                    <td>${tokens.toFixed(2)}</td>
                `;
                tbody.appendChild(tr);
            });

            // Add a non-breaking space to all headers
            document.querySelectorAll('#leaderboard th').forEach((headerCell) => {
                const baseText = headerCell.textContent.trim();
                headerCell.innerHTML = `${baseText}&nbsp;&nbsp;`;
            });
            const materialAdvantageHeader = document.querySelectorAll('#leaderboard th')[1];
            const defaultSortColumnIndex = 1; // Assuming this is defined above
            const defaultSortHeader = document.querySelectorAll('#leaderboard th')[defaultSortColumnIndex];
            defaultSortHeader.innerHTML = `${defaultSortHeader.textContent.trim()}&nbsp;▼`;
            document.querySelectorAll('#leaderboard th').forEach((headerCell, index) => {
                headerCell.addEventListener('click', () => {
                    sortTable(index);
                });
            });
        }

        function sortTable(columnIndex) {
            const table = document.querySelector('#leaderboard tbody');
            const rows = Array.from(table.rows);
            const isNumericColumn = columnIndex !== 0; // Assuming first column is not numeric

            // Determine the current sort order for the column
            const currentOrder = currentSortOrder[columnIndex] || 'asc';
            const newOrder = currentOrder === 'asc' ? 'desc' : 'asc';
            currentSortOrder[columnIndex] = newOrder;

            // Separate the rows that should always be at the bottom
            const bottomRows = [];
            const otherRows = rows.filter(row => {
                const player = row.cells[0].textContent;
                if (player === SPECIAL_ROWS.STOCKFISH ||
                    player === SPECIAL_ROWS.RANDOM_WHITE ||
                    player === SPECIAL_ROWS.RANDOM_BLACK) {
                    bottomRows.push(row);
                    return false;
                }
                return true;
            });

            otherRows.sort((a, b) => {
                const aText = a.cells[columnIndex].textContent;
                const bText = b.cells[columnIndex].textContent;

                const comparison = isNumericColumn
                    ? parseFloat(aText) - parseFloat(bText)
                    : aText.localeCompare(bText);

                return newOrder === 'asc' ? comparison : -comparison;
            });

            // Append sorted rows and then the bottom rows
            [...otherRows, ...bottomRows].forEach(row => table.appendChild(row));

            // Clear all sort indicators
            document.querySelectorAll('#leaderboard th').forEach((headerCell) => {
                const baseText = headerCell.textContent.replace(/[▲▼]/g, '').trim();
                headerCell.innerHTML = `${baseText}&nbsp;&nbsp;`;
            });

            // Update header text with sorting indicator for the sorted column
            const sortedHeaderCell = document.querySelectorAll('#leaderboard th')[columnIndex];
            const baseText = sortedHeaderCell.textContent.replace(/[▲▼]/g, '').trim();
            const indicator = currentSortOrder[columnIndex] === 'asc' ? '▲' : '▼';
            sortedHeaderCell.innerHTML = `${baseText}${indicator ? '&nbsp;' + indicator : '&nbsp;&nbsp;'}`;
        }
        document.addEventListener('DOMContentLoaded', () => {
            fillTableWithCSV();
            fetchAndAnimateBoard();
            // Set initial sort order for the default column
            currentSortOrder[defaultSortColumnIndex] = 'desc'; // Assuming "Draws %" is the second column
        });

        function fetchAndAnimateBoard() {
            fetch('moves.txt')
                .then(response => {
                    if (!response.ok) {
                        throw new Error('Network response was not ok');
                    }
                    return response.text();
                })
                .then(data => {
                    const boardStates = data.trim().split('-\n');
                    let currentIndex = 0;

                    let interval = 1000; // Initial interval in milliseconds
                    let resetGameInterval = 7000;

                    function animateBoard() {
                        document.querySelector('pre.board').textContent = boardStates[currentIndex].trim();
                        document.querySelector('div.game-over').style.display = 'none';
                        currentIndex = (currentIndex + 1) % boardStates.length;

                        if (currentIndex === 0) {
                            document.querySelector('div.game-over').style.display = 'block';
                            currentIndex = 0;
                            setTimeout(animateBoard, resetGameInterval);
                        } else {
                            // Decrease the interval progressively
                            interval = Math.max(50, interval - 25); // Decrease by 25ms, but not less than 50ms
                            setTimeout(animateBoard, interval);
                        }
                    }

                    animateBoard(); // Start the animation
                })
                .catch(error => {
                    console.error('Error fetching moves.txt:', error);
                });
        }
    </script>
</head>
<body>
    <div class="center">
        <pre class="title">
 __       __                    ____     __                               
/\ \     /\ \       /'\_/`\    /\  _``. /\ \                              
\ \ \    \ \ \     /\      \   \ \ \/\_\\ \ \___      __    ____    ____  
 \ \ \  __\ \ \  __\ \ \__\ \   \ \ \/_/_\ \  _ `\  /'__`\ /',__\  /',__\ 
  \ \ \L\ \\ \ \L\ \\ \ \_/\ \   \ \ \L\ \\ \ \ \ \/\  __//\__, `\/\__, `\
   \ \____/ \ \____/ \ \_\\ \_\   \ \____/ \ \_\ \_\ \____\/\____/\/\____/
    \/___/   \/___/   \/_/ \/_/    \/___/   \/_/\/_/\/____/\/___/  \/___/ 

        </pre>
        <pre class="title-narrow">
         __         __         __    __                        
        /\ \       /\ \       /\ "-./  \                       
        \ \ \____  \ \ \____  \ \ \-./\ \                      
         \ \_____\  \ \_____\  \ \_\ \ \_\                     
          \/_____/   \/_____/   \/_/  \/_/                                                                                
 ______     __  __     ______     ______     ______    
/\  ___\   /\ \_\ \   /\  ___\   /\  ___\   /\  ___\   
\ \ \____  \ \  __ \  \ \  __\   \ \___  \  \ \___  \  
 \ \_____\  \ \_\ \_\  \ \_____\  \/\_____\  \/\_____\ 
  \/_____/   \/_/\/_/   \/_____/   \/_____/   \/_____/ 
        </pre>
        <div class="game">
            <span style="color: yellow;">Random Player (White)</span>
            <div class="board">
<pre class="board">
♜ ♞ ♝ ♛ ♚ ♝ ♞ ♜
♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
♙ ♙ ♙ ♙ ♙ ♙ ♙ ♙
♖ ♘ ♗ ♕ ♔ ♗ ♘ ♖</pre>
                <div class="game-over">
                    <p>GAME OVER</p>
                    <span>
                        - Outcome: Draw<br>
                        - Max moves reached: 200<br>
                        - Material White: 16<br>
                        - Material Black: 18
                    </span>
                </div>
            </div>
            <span style="color: yellow;">GPT-4o Mini (Black)</span>
        </div>
        <p class="intro">
            Can a Large Language Model play chess? You can prompt it to make
            a move based on the board state, hint it with a list of legal moves (and ask to pick one). You will find that 
            an LLM can move pieces, even provide sound comments on why it made a certain move
            and what tactic or strategy it followed.
            <br/><br/>
            But can LLMs actually make any meaningful moves and win in a chess game? Why don't
            we put them up against a random player (i.e., a bot that randomly picks any move from a list of 
            legal moves for the current position). After all, the models are called Foundational Models for
            a reason; they have the knowledge of the entire Internet, can (supposedly) reason, and pass
            numerous math evaluations and PhD-level exams. What could be easier for an LLM than to score a victory over
            a chaos monkey?
            <br/><br/>
            Let's find out ツ
        </p>
        <div class="button-container">
            <button class="selected" onclick="showPane('leaderboard')">Leaderboard</button>
            <button onclick="showPane('how-it-works')">How it works</button>
            <button onclick="showPane('considerations')">Thoughts</button>
        </div>
        <div id="leaderboard" class="pane">
            <div class="table-container">
                <table border="0">
                    <thead>
                        <tr>
                            <th title="Model playing as black against a Random Player">Player&nbsp;&nbsp;</th>
                            <th title="How often the player scored a win (due to check mate or the opponent failing to make)">Wins&nbsp;&nbsp;</th>
                            <th title="Percentage of games without a winner (e.g. reaching maximum number of 200 moves OR stalemate). Displays LLMs proficiency in chess.">Draws&nbsp;&nbsp;</th>
                            <th title="Number of LLM erroneous replies per 1000 moves - how often did an LLM fail to follow the instructions and make a move, e.g. due haluscinations picking illegal move and not conforming to communication protocol. Shows model's instruction following capabilities and halusinations resistance">Mistakes&nbsp;&nbsp;</th>
                            <th title="Number of token's generated per one move. Demonstrates model's verbosity.">Tokens&nbsp;&nbsp;</th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- Data will be populated here -->
                    </tbody>
                </table>

            </div>
            <p class="descriptions">
                <strong>METRICS:</strong><br/><br/>
                <strong>- Player:</strong> Model playing as black against a Random Player.<br/>
                <strong>- Wins:</strong> How often the player scored a win (due to checkmate or the opponent failing to make a move). Displays LLMs' proficiency in chess.<br/>
                <strong>- Draws:</strong> Percentage of games without a winner (e.g., reaching the maximum number of 200 moves or stalemate). Displays weaks' LLMs' proficiency in chess if it can't win.<br/>
                <strong>- Mistakes:</strong> Number of LLM erroneous replies per 1000 moves - how often did an LLM fail to follow the instructions and make a move, e.g., due to hallucinations picking illegal moves and not conforming to the communication protocol. Shows the model's instruction-following capabilities and hallucination resistance.<br/>
                <strong>- Tokens:</strong> Number of tokens generated per move. Demonstrates the model's verbosity.<br/><br/>
                <strong>NOTES:</strong><br/><br/>
                - LLMs played as black against a Random Player (as white).<br/>
                - 30 game simulations for Random Player vs. LLM.<br/>
                - Bottom rows in green demonstrate how a Chess Engine (Stockfish v17) fares against a Random Player.<br/>
                - 1000 simulations for Random Player vs. Chess Engine and Random vs. Random.<br/>
                - <s>You see it right, LLMs scored 0 wins.</s> No longer the case, o1-mini being the 1st LLM scroging wins<br/>
                - <s>Using Draws instead of Wins to evaluate LLMs' chess proficiency</s>.<br/>
                - The default soritng is by Wins DESC, Draws DESC and Mistakes ASC <br/>
                - Strong models (those ones winning) are judged (in Chess proficiency) by % Won, weak ones - by % Draws <br/>
                - <strong>Mistakes</strong> metric gives an evaluation of LLMs' instruction-following capabilities and resistance to hallucinations (making up non-legal moves while having a list of legal moves provided in the prompt).<br/>
                - Sort by <strong>Mistakes</strong> column and get a ranking by instruction-following ability (models with the least mistakes being better) <br/>
            </p>
        </div>
        <div id="how-it-works" class="pane" style="display: none;">
            <p class="descriptions">
                <strong>Libraries and Dependencies Used:</strong><br/>
                - <strong>chess:</strong> A Python library for handling chess game rules and basic operations, including board representation, legal move evaluation, and game state evaluation. This is not a chess engine runnig the actual caclulation of the best move.<br/>
                - <strong>Microsoft Autogen</strong> is used as a backbone for LLM comunication. It also implements the interaction between a Chess Board and custom agents like GameAgent, RandomPlayerAgent, AutoReplyAgent, and others for simulating different player types.<br/>
                - <strong>Stockfish</strong> - the chess engine doing the actual best move calculation, used as a referrence to demonstrate what a real chess player's peformance is.<br/>
                <br/>
                <strong>Workflow:</strong><br/>
                1. The game is initialized with a chess board and two players: a Random Player (as white) and an LLM (as black).<br/>
                2. The game loop runs until a termination condition is met, such as checkmate, stalemate, or reaching the maximum number of moves.<br/>
                3. Each player takes turns making a move. The Random Player selects a move randomly from the list of legal moves.<br/>
                4. The LLM is prompted to make a move using a structured dialog, which includes actions like getting the current board state, retrieving legal moves, and making a move.<br/>
                5. The game state is updated after each move, and the board is visualized if enabled.<br/>
                6. Game statistics are generated and stored at the end of the game.<br/>
                <br/>
                <strong>Dialog:</strong><br/>
                Here is an example of a dialog that prompts a model to make a move:<br/>
                <pre class="snippet">
<span style="color:green">Proxy (to Player_Black):</span>
You are a professional chess player and you play as black. Now is your turn to make a move. Before making a move you can pick one of the following actions:
- 'get_current_board' to get the schema and current status of the board
- 'get_legal_moves' to get a UCI formatted list of available moves
- 'make_move <UCI formatted move>' when you are ready to complete your turn (e.g., 'make_move e2e4')
Respond with the action.

<span style="color:green">Player_Black (to Proxy):</span>
get_current_board

<span style="color:green">Proxy (to Player_Black):</span>
<span style="font-family: Chess;">♞ ♝ ♛ ♚ ♝ ♞ ♜ ♜
♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
♙ ♙ ♙ ♙ ♙ ♙ ♙ ♙
♖ ♘ ♗ ♕ ♔ ♗ ♘ ♖
</span>

<span style="color:green">Player_Black (to Proxy):</span>
get_legal_moves

<span style="color:green">Proxy (to Player_Black):</span>
a7a6, b7b6, c7c6, d7d6, e7e6, f7f6, g7g6, h7h6, a7a5, b7b5, c7c5, d7d5, e7e5, f7f5, g7g5, h7h5

<span style="color:green">Player_Black (to Proxy):</span>
make_move e7e5

<span style="color:green">Proxy (to Player_Black):</span>
Move made, switching player
                </pre>
            </p>
            <p class="descriptions">
                1 move - 1 dialog. The dialog is limmited to 10 turns (10 pairs of Proxy/Agent Request/Response messages). A maximum of 3 mistakes (not conforming to 
                communication notation, picking wrong action or wrong move) are allowed per dialog. 
                The game is terminanted and LLM is given a loss if the max turns OR of number of LLM mistakes is reached in the given dialog.
            </p>
        </div>
        <div id="considerations" class="pane" style="display: none;">
            <p class="descriptions">
            Language Models can make moves in a chess game, but they cannot actually play chess or progress towards a victory. 
            The experiment started after taking a class at DeepLearning, which taught the Microsoft Autogen framework. One of the classes demonstrated
            a simulation of a chess game between LLMs. I was immediately intrigued by the idea of putting different LLMs head-to-head in a chess game competition. 
            However, I was surprised that the naive prompting strategy from the class never led to a game completion. Extending prompts didn't help. 
            I ended up testing LLMs' performance in a chess game using a Random Player. A human player with reasonable chess skills would have no problems winning 
            against a random player, yet LLMs failed miserably in this competition.
            <br/><br/>
            I suspect that crushing this "LLM Chess" eval might be as hard as the <a href="https://arcprize.org">ARC Challenge</a>—a benchmark created to demonstrate 
            the true nature of text-generating LLMs, exploit their weaknesses, and show how LLMs struggle with reasoning. "It's easy for humans, but hard for AI."
            <br/><br/>
            LLMs and Transformer-based models can be trained specifically to play chess. There are projects on the internet where people have fine-tuned LLMs
            as chess players. Yet, those are specialized models that can't be good chat models.
            <br/><br/>
            I am looking forward to testing new releases of SOTA and frontier models. It would be great to see a model that starts scoring wins against
            the chaos monkey while maintaining performance at traditional chat tasks.
            <br/>
            <br/>
            <strong>NOTES:</strong><br/>
            - More data on game simulations is available <a href="https://github.com/maxim-saplin/llm_chess/blob/main/docs/_data/refined.csv">here</a> and 
            <a href="https://github.com/maxim-saplin/llm_chess/blob/main/docs/_data/aggregate_models.numbers">here</a>.<br/>
            - No history of moves is available to LLM, no reflection used (giving the model "time to think").<br/>
            &nbsp;- Experiments with reflection suggest that LLMs do even worse when they are prompted to evaluate options before making a move
            (<a href="https://github.com/maxim-saplin/llm_chess/blob/main/_logs/_not_OK_logs/reflection/aggregate_results.csv">reflection results</a>).
            &nbsp;- Could LLMs improve their performance if given the whole log of the game?<br/>
            - The chess engine ( <a href="https://stockfishchess.org">Stockfish</a>) has a 100% win rate with an average game taking 32 moves to complete.<br/>
            &nbsp;- Stockfish 17, macOS, 0.1ms time limit (vs 0.1s default) - with decreased performance, Stockfish dominates over Random Player.<br/>
            - Random Player (as white) wins over Random Player (as black) in 10.5% of cases - LLMs scored 0 wins.<br/>
            &nbsp;- Indeed, giving the right of the first move gives an advantage to the white player.<br/>
            &nbsp;- LLMs do worse than random players.<br/>
            &nbsp;- It's as if an LLM had no goal to win, as if it was its intention to keep the game going. What if I prompted it and told it that 200 moves is the maximum and the game ends after? Would it try harder? Can adding to a system prompt an explicitly instruction to Win help?<br/>
            - While some models are less verbose and follow the rules strictly (e.g., OpenAI), others are verbose.<br/>
            &nbsp;- Initially, I used exact match when communicating with an LLM and prompted it to reply with action names (show board, get legal moves, make move) - worked well with OpenAI.<br/>
            &nbsp;- After the list of models was extended, the original prompts had issues steering them.<br/>
            &nbsp;- As a result, I changed the communication protocol to use regex and be tolerant to reply format, doing its best to extract action and arguments from LLM replies.<br/>
            - Since models don't score any Wins there must be some alternative metric demosntrating game progress.<br/>
            &nbsp;- For the time being using Draws, the more draws - the better.<br/>
            &nbsp;- Yet most of the draws scored are due to hitting the 200 max moves limmit and hence the metric demonstrates the adherence to communication protocol/prompt cpnverntions.<br/>
            &nbsp;- Logs also contain "Material Count" metrics - the weighted scores of piaces a player has, at the beginning a player has a total of 39 units of material.<br/>
            &nbsp;- Material difference could be a good metric to evluate progress, the player having more material left as the game progress is at a better position.<br/>
            &nbsp;- Yet most of the models demonstrated negative material difference and one of the models (gpt-35-turbo-0125) failed to make a single move having a material difference at 0 putting it above models that had negative material while staying in the game much longer.<br/>
            &nbsp;- It might be resonable to create a computed metric that account for both the material and length of the game, addressing the endless pointless game concern, as well as never changing material diff due to failing to early.<br/>
            - What if the model is not given the list of legal moves? Will the model figuring out legal moves on their own and struggle to progress? Can giving the models a list of legal moves essentially break reasoning turning the game into simple instruction following (i.e. pick one item from the list rather than win in the game)?
            - The older GPT-4 Turbo did better than newer GPT-4o version, this is a yet another eval demonstrating how newer models performed worse supporting the claim the the 4o family of models are smaller and cheaper to run models.<br/>
        </div>
        <a class="gh" href="https://github.com/maxim-saplin/llm_chess/" target="_blank">
            Project's GitHub
        </a>
    </div>
</body>
</html>
