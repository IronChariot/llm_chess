<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="apple-touch-icon" sizes="180x180" href="icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="icons/favicon-16x16.png">
    <link rel="icon" type="image/x-icon" href="icons/favicon.ico">
    <title>LLM Chess Leaderboard</title>
    <style>
        @font-face {
            font-family: 'Web IBM VGA 8x16';
            font-style: normal;
            font-weight: 400;
            src: url(web_ibm_vga_8x16.woff) format('woff');
        }

        @font-face {
            font-family: 'Chess';
            font-style: normal;
            font-weight: 400;
            /* src: url(segoe-w95-chess.woff) format('woff'); */
            src: url(DejavuSansMono.ttf) format('truetype');
        }

        body {
            background-color: #0000AA;
            color: #FFFFFF;
            font-family: "Web IBM VGA 8x16";
            text-align: center;
            margin: 0;
            font-size: 20px;
            overflow-y: scroll;
        }

        code {
            font-size: 20px;
            font-family: "Web IBM VGA 8x16";
            color: white;
        }

        div.center {
            width: 740px;
            margin: auto;
            padding-bottom: 30px;
            padding-left: 20px;
            padding-right: 20px;
        }

        a {
            color: greenyellow;
            text-decoration: none; 
        }

        .gh {
            color: yellowgreen;
        }

        .gh img {
            vertical-align: middle;
            width: 20px;
            height: 20px;
            margin-right: 5px;
            filter: invert(100%);
        }

        h1 {
            margin-bottom: 20px;
        }

        p.intro {
            display: block;
            margin:auto;
            text-align: justify;
        }

        .button-container {
            display: flex;
            justify-content: center;
            margin-top: 30px;
            margin-bottom: 40px;
        }

        button {
            width: 160px;
            margin: 0 5px;
            padding: 5px;
            background-color: #C0C0C0;
            color: #000000;
            border: 1px solid #808080;
            font-family: "Web IBM VGA 8x16";
            font-size: 20px;
            position: relative;
            cursor: pointer;
        }

        button::after {
            content: '';
            position: absolute;
            top: 8px;
            left: 8px;
            width: 100%;
            height: 100%;
            background-color: #000000;
            z-index: -1;
        }

        button.selected {
            background-color: green;
            color: #FFFFFF;
            top: 1px;
            left: 1px;
        }

        button:hover {
            background-color: #808080;
            color: #FFFFFF;
            top: 2px;
            left: 2px;
        }

        .pane{
            text-align: left;
        }

        pre.title {
            font-size: 16px;
            scale: 1.0;
            padding-top: 50px;
            color: greenyellow;
        }

        pre.snippet {
            font-family: "Web IBM VGA 8x16";
            text-align: left;
            color: greenyellow;
            overflow: auto;
        }

        pre.title-narrow {
            display: none;
        }

        .game {
            padding-bottom: 50px;
        }

        pre.board {
            font-family: 'Chess';
            font-size: 32px;
            width: 295px;
            height: 298px;
            background-color: black;
            text-align: justify;
            opacity: 0.85;
            line-height: 37px;
            margin: 0;
            padding-left: 5px;
            padding-top: 2px;
        }

        div.board::before {
            content: " ";
            display: block;
            position: absolute;
            top: 0;
            left: 0;
            bottom: 0;
            right: 0;
            background: linear-gradient(rgba(18, 16, 16, 0) 50%, rgba(0, 0, 0, 0.25) 50%), linear-gradient(90deg, rgba(255, 0, 0, 0.06), rgba(0, 255, 0, 0.02), rgba(0, 0, 255, 0.06));
            z-index: 2;
            background-size: 100% 2px, 3px 100%;
            pointer-events: none;
        }

        div.board {
            width: 300px;
            height: 300px;
            padding: 5px;
            margin-top: 20px;
            margin-bottom: 20px;
            margin-left: auto;
            margin-right: auto;
            position: relative;
        }

        .game-over {
            position: absolute;
            top: 0;
            left: 0;
            background-color: rgba(0, 0, 0, .9);
            margin: 5px;
            width: 300px;
            height: 300px;
            text-align: center;
            display: none;
        }

        @keyframes blink {
            0%, 100% {
                opacity: 1;
            }
            50% {
                opacity: 0;
            }
        }
        
        .game-over p {
            color: red;
            font-size: 40px;
            font-weight: bold;
            animation: blink 1s infinite;
        }

        .game-over span {
            text-align: left;
            display: inline-flex;
        }

        p, pre {
            color: lightgray;
        }

        .table-container {
            overflow-x: auto;
            padding: 0;
        }

        table {
            margin-bottom: 40px;
            min-width: 720px;
            width: 100%;
            border-collapse: collapse;
            border: 0;
        }
        
        td, th {
            padding: 10px;
            font-weight: lighter;
            text-align: left;
        }

        th {
            cursor: pointer;
        }

        th:hover {
            cursor: pointer;
            color: yellowgreen;
        }

        tr:hover td{
            color: yellowgreen;
        }

        tr:hover td:nth-child(even) {
            color: darkgreen;
        }

        td {
            border-top: 1px solid;
            cursor: default;
            color: lightgray;
        }

        th:nth-child(even), td:nth-child(even) {
            background-color: lightgray;
            color: black;
            box-shadow: 8px 8px;
        }

        table tbody tr:nth-last-child(-n+3) td {
            color: greenyellow !important;
            background-color: transparent;
            box-shadow: 0 0;
        }

        .descriptions {
            text-align: left;
            padding-top: 0;
        }
        
        .descriptions strong {
            font-weight: normal;
            color: white;
        }

        #popup {
            display: none;
            position: absolute;
            background-color: #333;
            color: #fff;
            z-index: 1000;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);
            padding: 10px;
            border-radius: 5px;
            text-align: left;
        }
        
        #popup p {
            margin: 0;
        }

        /* Responsive styles for mobile devices */
        @media (max-width: 768px) {
            body {
                font-size: 18px;
            }

            div.center {
                width: 90%;
            }

            h1 {
                font-size: 24px;
            }

            p.intro {
                width: 100%;
            }

            div.board {
                scale: 0.9;
            }

            button {
                width: 140px;
                font-size: 18px;
            }

            pre.title {
                display: none;
            }

            pre.title-narrow {
                display: block;
                font-size: 10px;
                padding-top: 20px;
                padding-bottom: 20px;
                color: white;
                max-width: 325px;
                overflow-x: hidden;
                margin: auto;
                line-height: 1.3;
                color: greenyellow;
            }

            td, th {
                padding: 8px;
            }

            .button-container {
                flex-direction: column;
                align-items: center;
            }

            .button-container button {
                margin: 5px 0;
            }
        }
    </style>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-4SS6VHS4QF"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4SS6VHS4QF');
    </script>
    <script>
        const data = `
anthropic.claude-v3-5-sonnet,30,0,8,22,0.0,73.33333333333333,5148,0,12,15.8,22.5,-6.7,-36.953723217813106,0.0,14.657738095238095,171.6,81.07478632478633
anthropic.claude-v3-5-sonnet-v1,60,4,8,48,6.666666666666667,80.0,11003,0,3,13.2,17.366666666666667,-4.166666666666667,-19.325398324516605,0.0,1.6666666666666665,183.38333333333333,80.42406616377352
anthropic.claude-v3-5-sonnet-v2,60,2,5,53,3.3333333333333335,88.33333333333333,11292,0,8,10.816666666666666,13.833333333333334,-3.0166666666666666,-5.1134194118105745,0.0,2.9828722002635044,188.2,90.8544987601842
anthropic.claude-v3-haiku,40,0,40,0,0.0,0.0,1334,7,4,36.725,37.15,-0.425,-11.710852516809853,13.203463203463201,13.541666666666664,33.35,210.64017991004496
anthropic.claude-v3-opus,30,0,5,25,0.0,83.33333333333334,4968,1,7,15.633333333333333,21.633333333333333,-6.0,-33.49803552340468,1.8518518518518516,7.457634521313766,165.6,72.8623188405797
deepseek-chat-v3,70,0,68,2,0.0,2.857142857142857,4043,10,180,32.17142857142857,30.52857142857143,1.6428571428571428,35.272572014318335,1.9779146503284433,96.34064766615487,57.75714285714286,246.92604501607718
deepseek-r1-distill-qwen-14b@q8_0,30,0,30,0,0.0,0.0,78,82,8,39.0,39.0,0.0,0.0,1175.0,116.66666666666666,2.6,3073.0641025641025
deepseek-r1-distill-qwen-32b@q4_k_m,30,0,30,0,0.0,0.0,110,73,7,39.0,38.96666666666667,0.03333333333333333,3.333333333333333,894.4444444444445,65.83333333333333,3.6666666666666665,2173.827272727273
deepseek-reasoner-r1,31,7,18,6,22.58064516129032,19.35483870967742,2845,9,44,21.93548387096774,10.806451612903226,11.129032258064516,170.38444469706076,6.10574660050437,46.825282688770564,91.7741935483871,4584.972583479789
gemini-1.5-flash-001,30,0,20,10,0.0,33.33333333333333,2524,18,36,29.266666666666666,35.666666666666664,-6.4,-33.14049391468746,114.57671957671957,229.15343915343914,84.13333333333334,19.914817749603802
gemini-1.5-pro-preview-0409,40,0,37,3,0.0,7.5,2626,14,71,29.75,33.075,-3.325,-38.3699709622505,4.7327303719316784,77.71717780731329,65.65,13.383472962680884
gemini-2.0-flash-exp,30,0,28,2,0.0,6.666666666666667,2576,0,81,26.933333333333334,27.166666666666668,-0.23333333333333334,-12.193564485896013,0.0,117.39474439973138,85.86666666666666,168.15023291925465
gemini-2.0-flash-thinking-exp-01-21,33,0,33,0,0.0,0.0,1341,7,1,33.21212121212121,33.72727272727273,-0.5151515151515151,-7.6864192192695455,49.62982093663911,0.6184291898577613,40.63636363636363,17.76957494407159
gemini-2.0-flash-thinking-exp-1219,30,0,30,0,0.0,0.0,70,89,1,39.0,39.0,0.0,0.0,1386.111111111111,5.5555555555555545,2.3333333333333335,724.5428571428571
gemma-2-27b-it@q6_k_l,30,0,22,8,0.0,26.666666666666668,3268,8,52,25.466666666666665,26.333333333333332,-0.8666666666666667,13.392604995286117,3.8314076492517968,48.07826143763394,108.93333333333334,55.04436964504284
gemma-2-9b-it-8bit,30,0,26,4,0.0,13.333333333333334,2075,45,31,32.0,34.13333333333333,-2.1333333333333333,-30.559760306903236,163.66922692296473,71.24054344481173,69.16666666666667,58.12433734939759
gemma2-9b-it,35,0,35,0,0.0,0.0,516,83,22,38.714285714285715,38.31428571428572,0.4,11.613037559521416,414.8091421576605,61.74548926428625,14.742857142857142,20.217054263565892
gpt-35-turbo-0125,30,0,30,0,0.0,0.0,86,90,0,39.0,39.0,0.0,0.0,1285.0,0.0,2.8666666666666667,82.02325581395348
gpt-35-turbo-0301,30,0,30,0,0.0,0.0,68,90,0,39.0,39.0,0.0,0.0,1400.0,0.0,2.2666666666666666,67.05882352941177
gpt-35-turbo-0613,30,0,30,0,0.0,0.0,124,90,0,39.0,38.96666666666667,0.03333333333333333,2.7777777777777772,1051.6666666666667,0.0,4.133333333333334,93.62903225806451
gpt-35-turbo-1106,30,0,30,0,0.0,0.0,108,88,2,39.0,39.0,0.0,0.0,1130.2777777777778,8.88888888888889,3.6,48.324074074074076
gpt-4-turbo-2024-04-09,30,0,2,28,0.0,93.33333333333333,5786,0,0,15.433333333333334,20.8,-5.366666666666666,-30.62647754137116,0.0,0.0,192.86666666666667,6.032837884548911
gpt-4o-2024-05-13,60,0,12,48,0.0,80.0,11057,1,16,13.633333333333333,17.35,-3.716666666666667,-15.277597049390701,0.08333333333333333,3.461510791480984,184.28333333333333,31.334720086822827
gpt-4o-2024-08-06,60,1,9,50,1.6666666666666667,83.33333333333334,11214,0,1,14.483333333333333,19.333333333333332,-4.85,-32.06579503621741,0.0,0.15723270440251572,186.9,7.70322810772249
gpt-4o-2024-11-20,71,3,6,62,4.225352112676056,87.32394366197182,13470,1,1,11.901408450704226,19.901408450704224,-8.0,-41.770473664269865,0.07042253521126761,0.07042253521126761,189.71830985915494,50.575278396436524
gpt-4o-mini-2024-07-18,30,0,12,18,0.0,60.0,4481,13,24,20.433333333333334,24.866666666666667,-4.433333333333334,-38.65782480051881,4.701451970801816,31.083346306256523,149.36666666666667,108.21624637357732
granite-3.1-8b-instruct,30,0,30,0,0.0,0.0,126,47,12,38.96666666666667,39.0,-0.03333333333333333,-4.166666666666667,447.5,91.66666666666666,4.2,469.12698412698415
grok-2-1212,49,2,32,15,4.081632653061225,30.612244897959183,5593,1,93,25.3265306122449,22.387755102040817,2.938775510204082,42.245054095055764,0.5102040816326531,32.28054641491559,114.14285714285714,66.22867870552476
internlm3-8b-instruct,30,0,30,0,0.0,0.0,108,15,39,39.0,39.0,0.0,0.0,166.66666666666669,412.5,3.6,1543.898148148148
llama-2-7b-chat,30,0,30,0,0.0,0.0,64,88,2,39.0,39.0,0.0,0.0,1425.0,25.0,2.1333333333333333,116.3125
llama-3-70b-instruct-awq,30,0,15,15,0.0,50.0,4449,2,41,21.566666666666666,26.966666666666665,-5.4,-21.856163100745917,0.606060606060606,22.27814337138526,148.3,41.60890087660148
llama-3.1-8b-instant,60,0,60,0,0.0,0.0,1754,27,146,36.61666666666667,37.53333333333333,-0.9166666666666666,-29.803101898153308,30.761016391498845,142.10738044440947,29.233333333333334,166.50513112884835
llama-3.3-70b,42,0,38,4,0.0,9.523809523809524,2886,2,100,30.904761904761905,33.45238095238095,-2.5476190476190474,-12.651372376559515,3.3068783068783065,107.34967736957964,68.71428571428571,102.98163548163548
llama3-8b-8192,60,0,60,0,0.0,0.0,902,104,66,38.2,38.68333333333333,-0.48333333333333334,-14.169665566917889,206.9415314217946,89.71030395043553,15.033333333333333,57.019955654102
llama3.1-8b,90,0,87,3,0.0,3.3333333333333335,2436,44,188,37.2,37.833333333333336,-0.6333333333333333,-15.745408923494072,35.03988017468409,126.64663826670215,27.066666666666666,162.1013957307061
meta-llama-3.1-8b-instruct-fp16,30,0,30,0,0.0,0.0,830,19,61,36.733333333333334,37.0,-0.26666666666666666,4.630161770966371,72.35501567398119,135.4742208621519,27.666666666666668,71.86024096385542
ministral-8b-instruct-2410,30,0,30,0,0.0,0.0,282,56,34,38.766666666666666,38.6,0.16666666666666666,13.888888888888886,453.517871017871,152.81204906204906,9.4,72.11347517730496
mistral-nemo-12b-instruct-2407,30,0,30,0,0.0,0.0,202,79,11,38.766666666666666,38.666666666666664,0.1,13.75,733.9484126984127,42.5,6.733333333333333,47.698019801980195
mistral-small-24b-instruct-2501@q4_k_m,42,0,42,0,0.0,0.0,854,3,123,37.404761904761905,37.73809523809524,-0.3333333333333333,-4.2997843174401345,4.8840048840048835,221.71666727807047,20.333333333333332,110.94847775175644
mistral-small-instruct-2409,30,0,30,0,0.0,0.0,272,58,32,38.86666666666667,38.833333333333336,0.03333333333333333,4.318181818181818,365.4960317460318,127.91847041847042,9.066666666666666,88.24264705882354
o1-mini-2024-09-12,30,9,6,15,30.0,50.0,4282,2,8,17.666666666666668,7.033333333333333,10.633333333333333,94.87098362233894,0.9450830140485312,3.3492881937409673,142.73333333333332,1221.1361513311538
o1-preview-2024-09-12,30,14,3,13,46.666666666666664,43.333333333333336,3744,4,10,17.8,4.033333333333333,13.766666666666667,182.50560050089678,2.9231524976205825,6.364050790877428,124.8,2660.071848290598
phi-4,30,0,30,0,0.0,0.0,232,44,46,39.0,39.0,0.0,0.0,264.8484848484849,232.67316017316017,7.733333333333333,333.5431034482759
qwen-max-2025-01-25,60,0,2,58,0.0,96.66666666666667,11790,0,0,18.533333333333335,21.383333333333333,-2.85,-16.695668001456134,0.0,0.0,196.5,6.06234096692112
qwen-plus-2025-01-25,33,0,30,3,0.0,9.090909090909092,2890,2,89,28.515151515151516,25.90909090909091,2.606060606060606,50.09424512759104,1.1784511784511784,64.85346359080525,87.57575757575758,440.41384083044983
qwen-turbo-2024-11-01,33,0,33,0,0.0,0.0,674,4,95,38.0,38.333333333333336,-0.3333333333333333,-21.689522561731494,10.267145135566189,220.39542203400208,20.424242424242426,192.3679525222552
qwen2.5-14b-instruct@q8_0,30,0,30,0,0.0,0.0,398,29,59,38.56666666666667,38.4,0.16666666666666666,3.192640692640695,156.93121693121694,212.67376142376145,13.266666666666667,150.63065326633165
qwen2.5-72b-instruct,30,0,28,2,0.0,6.666666666666667,1923,10,71,30.666666666666668,32.733333333333334,-2.066666666666667,-23.86190778620155,8.232851372597288,81.14434750949682,64.1,219.46541861674467
qwen2.5-7b-instruct-1m,42,0,42,0,0.0,0.0,406,33,93,38.73809523809524,38.714285714285715,0.023809523809523808,3.9694102194102188,180.3773276987563,403.0556084127512,9.666666666666666,140.79064039408868
qwq-32b-preview@q4_k_m,30,0,30,0,0.0,0.0,239,44,17,38.86666666666667,38.96666666666667,-0.1,-13.068181818181818,308.6381673881674,97.35569985569984,7.966666666666667,2908.0
sky-t1-32b-preview,30,0,30,0,0.0,0.0,415,9,59,38.766666666666666,38.8,-0.03333333333333333,-0.8086036346905914,24.570707070707073,185.1043159738812,13.833333333333334,1216.1590361445783
Stockfish chess engine (as Black),1000,1000,0,0,100.000,0.000,32368,0,0,38.21,21.01,17.21,0.53164,0.00,0.00,32.37,-
Random Player (as White),1000,105,0,895,10.5,89.5,190073,0,0,10.56,11.08,-0.52,-0.00274,0.00,0.00,190.07,-
Random Player (as Black),1000,0,105,895,0.000,89.5,190073,0,0,11.08,10.56,0.52,0.00274,0.00,0.00,190.07,-
            `
        const csvIndices = {
            player: 0,
            total_games: 1,
            player_wins: 2,
            losses: 3,
            draws: 4,
            player_wins_percent: 5,
            player_draws_percent: 6,
            wrong_actions_per_1000moves: 14,
            wrong_moves_per_1000moves: 15,
            completion_tokens_black_per_move: 17,
            average_moves: 16,
            material_diff: 12,

        };

        let currentSortOrder = {};

        const SPECIAL_ROWS = {
            STOCKFISH: "Stockfish chess engine (as Black)",
            RANDOM_WHITE: "Random Player (as White)",
            RANDOM_BLACK: "Random Player (as Black)"
        };

        document.addEventListener('DOMContentLoaded', () => {
            buildTable();
            fetchAndAnimateBoard();
        });

        function showPane(paneId) {
            document.getElementById('leaderboard').style.display = paneId === 'leaderboard' ? 'block' : 'none';
            document.getElementById('how-it-works').style.display = paneId === 'how-it-works' ? 'block' : 'none';
            document.getElementById('considerations').style.display = paneId === 'considerations' ? 'block' : 'none';
            document.querySelectorAll('.button-container button').forEach(button => {
                button.classList.remove('selected');
            });
            document.querySelector(`.button-container button[onclick="showPane('${paneId}')"]`).classList.add('selected');

            // Simulate a page view event in Google Analytics
            gtag('event', 'page_view', {
                'page_title': document.title + ' - ' + paneId,
                'page_path': '/' + paneId
            });
        }

        function buildTable() {
            const lines = data.trim().split('\n');
            const header = lines[0].split(',');
            const rows = lines.slice(1).map(row => row.split(','));

            // Separate the rows that should always be at the bottom
            const bottomRows = [];
            const otherRows = rows.filter(columns => {
                const player = columns[csvIndices.player];
                if (player === SPECIAL_ROWS.STOCKFISH ||
                    player === SPECIAL_ROWS.RANDOM_WHITE ||
                    player === SPECIAL_ROWS.RANDOM_BLACK) {
                    bottomRows.push(columns);
                    return false;
                }
                return true;
            });

            // Sort the remaining rows by "Wins", "Draws", and then "Mistakes" in descending order
            otherRows.sort((a, b) => {
                const winsA = parseFloat(a[csvIndices.player_wins_percent]);
                const winsB = parseFloat(b[csvIndices.player_wins_percent]);
                const drawsA = parseFloat(a[csvIndices.player_draws_percent]);
                const drawsB = parseFloat(b[csvIndices.player_draws_percent]);
                const mistakesA = parseFloat(a[csvIndices.wrong_actions_per_1000moves]) + parseFloat(a[csvIndices.wrong_moves_per_1000moves]);
                const mistakesB = parseFloat(b[csvIndices.wrong_actions_per_1000moves]) + parseFloat(b[csvIndices.wrong_moves_per_1000moves]);

                // Hierarchical sorting: Wins DESC, Draws DESC, Mistakes ASC
                return winsB - winsA || drawsB - drawsA || mistakesA - mistakesB;
            });

            const tbody = document.querySelector('#leaderboard tbody');
            [...otherRows, ...bottomRows].forEach(columns => {
                const player = columns[csvIndices.player];
                const player_wins_percent = parseFloat(columns[csvIndices.player_wins_percent]);
                const player_draws_percent = parseFloat(columns[csvIndices.player_draws_percent]);
                const wrong_actions_per_1000moves = parseFloat(columns[csvIndices.wrong_actions_per_1000moves]);
                const wrong_moves_per_1000moves = parseFloat(columns[csvIndices.wrong_moves_per_1000moves]);
                const mistakes = wrong_actions_per_1000moves + wrong_moves_per_1000moves;
                const tokens = parseFloat(columns[csvIndices.completion_tokens_black_per_move]);

                const tr = document.createElement('tr');
                tr.innerHTML = `
                    <td>${player}</td> <!-- Player first -->
                    <td>${player_wins_percent.toFixed(2)}%</td>
                    <td>${player_draws_percent.toFixed(2)}%</td>
                    <td>${mistakes.toFixed(2)}</td>
                    <td>${tokens.toFixed(2)}</td>
                `;
                tbody.appendChild(tr);

                // Add event listeners for hover and tap
                tr.addEventListener('mouseenter', () => showPopup(tr, columns));
                tr.addEventListener('mouseleave', hidePopup);
                tr.addEventListener('click', () => showPopup(tr, columns));
            });

            // Add a non-breaking space to all headers
            document.querySelectorAll('#leaderboard th').forEach((headerCell) => {
                const baseText = headerCell.textContent.trim();
                headerCell.innerHTML = `${baseText}&nbsp;&nbsp;`;
            });

            document.querySelectorAll('#leaderboard th').forEach((headerCell, index) => {
                headerCell.addEventListener('click', () => {
                    sortTable(index);
                });
            });
        }

        function sortTable(columnIndex) {
            const table = document.querySelector('#leaderboard tbody');
            const rows = Array.from(table.rows);
            const isNumericColumn = columnIndex !== 0; // Assuming first column is not numeric

            // Determine the current sort order for the column
            const currentOrder = currentSortOrder[columnIndex] || 'asc';
            const newOrder = currentOrder === 'asc' ? 'desc' : 'asc';
            currentSortOrder[columnIndex] = newOrder;

            // Separate the rows that should always be at the bottom
            const bottomRows = [];
            const otherRows = rows.filter(row => {
                const player = row.cells[0].textContent;
                if (player === SPECIAL_ROWS.STOCKFISH ||
                    player === SPECIAL_ROWS.RANDOM_WHITE ||
                    player === SPECIAL_ROWS.RANDOM_BLACK) {
                    bottomRows.push(row);
                    return false;
                }
                return true;
            });

            otherRows.sort((a, b) => {
                const aText = a.cells[columnIndex].textContent;
                const bText = b.cells[columnIndex].textContent;

                const comparison = isNumericColumn
                    ? parseFloat(aText) - parseFloat(bText)
                    : aText.localeCompare(bText);

                return newOrder === 'asc' ? comparison : -comparison;
            });

            // Append sorted rows and then the bottom rows
            [...otherRows, ...bottomRows].forEach(row => table.appendChild(row));

            // Clear all sort indicators
            document.querySelectorAll('#leaderboard th').forEach((headerCell) => {
                const baseText = headerCell.textContent.replace(/[▲▼]/g, '').trim();
                headerCell.innerHTML = `${baseText}&nbsp;&nbsp;`;
            });

            // Update header text with sorting indicator for the sorted column
            const sortedHeaderCell = document.querySelectorAll('#leaderboard th')[columnIndex];
            const baseText = sortedHeaderCell.textContent.replace(/[▲▼]/g, '').trim();
            const indicator = currentSortOrder[columnIndex] === 'asc' ? '▲' : '▼';
            sortedHeaderCell.innerHTML = `${baseText}${indicator ? '&nbsp;' + indicator : '&nbsp;&nbsp;'}`;
        }

        function fetchAndAnimateBoard() {
            fetch('moves.txt')
                .then(response => {
                    if (!response.ok) {
                        throw new Error('Network response was not ok');
                    }
                    return response.text();
                })
                .then(data => {
                    const boardStates = data.trim().split('-\n');
                    let currentIndex = 0;

                    let interval = 1000; // Initial interval in milliseconds
                    let resetGameInterval = 7000;

                    function animateBoard() {
                        document.querySelector('pre.board').textContent = boardStates[currentIndex].trim();
                        document.querySelector('div.game-over').style.display = 'none';
                        currentIndex = (currentIndex + 1) % boardStates.length;

                        if (currentIndex === 0) {
                            document.querySelector('div.game-over').style.display = 'block';
                            currentIndex = 0;
                            setTimeout(animateBoard, resetGameInterval);
                        } else {
                            // Decrease the interval progressively
                            interval = Math.max(50, interval - 25); // Decrease by 25ms, but not less than 50ms
                            setTimeout(animateBoard, interval);
                        }
                    }

                    animateBoard(); // Start the animation
                })
                .catch(error => {
                    console.error('Error fetching moves.txt:', error);
                });
        }

        function showPopup(row, columns) {
            const popup = document.getElementById('popup');
            const totalGames = columns[csvIndices.total_games];
            const wins = columns[csvIndices.player_wins]; // Retrieve wins
            const losses = columns[csvIndices.losses];    // Retrieve losses
            const draws = columns[csvIndices.draws];      // Retrieve draws
            const averageMoves = columns[csvIndices.average_moves]; // Retrieve average moves
            const materialDiff = columns[csvIndices.material_diff]; // Retrieve material difference

            document.getElementById('total-games').textContent = `Games: ${parseInt(totalGames)}`;
            document.getElementById('wins').textContent = `Wins: ${parseInt(wins)} | ${((parseInt(wins) / parseInt(totalGames)) * 100).toFixed(2)}%`;
            document.getElementById('losses').textContent = `Losses: ${parseInt(losses)} | ${((parseInt(losses) / parseInt(totalGames)) * 100).toFixed(2)}%`;
            const winsMinusLossesPercent = ((parseInt(wins) - parseInt(losses)) / parseInt(totalGames) * 100).toFixed(2);
            document.getElementById('wins_minus_losses').textContent = `Wins - Losses: ${winsMinusLossesPercent}%`;
            document.getElementById('draws').textContent = `Draws: ${parseInt(draws)} | ${((parseInt(draws) / parseInt(totalGames)) * 100).toFixed(2)}%`;
            document.getElementById('average-moves').textContent = `Average Moves: ${parseFloat(averageMoves).toFixed(2)}`;
            document.getElementById('material-diff').textContent = `Material Diff: ${parseFloat(materialDiff).toFixed(2)}`;

            const rect = row.getBoundingClientRect();
            if (window.innerWidth < 1200) {
                popup.style.top = `${rect.bottom + window.scrollY}px`;                
                popup.style.right = `${rect.left + window.scrollX}px`;
                popup.style.left = '';
            }
            else {
                popup.style.top = `${rect.top + window.scrollY}px`;
                popup.style.left = `${rect.right + window.scrollX}px`;
                popup.style.right = '';
            }
            popup.style.display = 'block';
        }

        function hidePopup() {
            const popup = document.getElementById('popup');
            popup.style.display = 'none';
        }

        function toggleSnippet(button) {
            const snippet = button.nextElementSibling; // Get the <pre> element
            if (snippet.style.display === "none") {
                snippet.style.display = "block";
                button.textContent = "Hide Snippet";
            } else {
                snippet.style.display = "none";
                button.textContent = "Show Snippet";
            }
        }
    </script>
</head>
<body>
    <p style="display: none;">
        LLM Chess Leaderboard
        <br>
        Simulating chess games between a Random Player and an LLM. Evaluating Chat Models' (1) chess proficiency and (2) instruction following abilities
    </p>
    <div class="center">
        <pre class="title">
 __       __                    ____     __                               
/\ \     /\ \       /'\_/`\    /\  _``. /\ \                              
\ \ \    \ \ \     /\      \   \ \ \/\_\\ \ \___      __    ____    ____  
 \ \ \  __\ \ \  __\ \ \__\ \   \ \ \/_/_\ \  _ `\  /'__`\ /',__\  /',__\ 
  \ \ \L\ \\ \ \L\ \\ \ \_/\ \   \ \ \L\ \\ \ \ \ \/\  __//\__, `\/\__, `\
   \ \____/ \ \____/ \ \_\\ \_\   \ \____/ \ \_\ \_\ \____\/\____/\/\____/
    \/___/   \/___/   \/_/ \/_/    \/___/   \/_/\/_/\/____/\/___/  \/___/ 

        </pre>
        <pre class="title-narrow">
         __         __         __    __                        
        /\ \       /\ \       /\ "-./  \                       
        \ \ \____  \ \ \____  \ \ \-./\ \                      
         \ \_____\  \ \_____\  \ \_\ \ \_\                     
          \/_____/   \/_____/   \/_/  \/_/                                                                                
 ______     __  __     ______     ______     ______    
/\  ___\   /\ \_\ \   /\  ___\   /\  ___\   /\  ___\   
\ \ \____  \ \  __ \  \ \  __\   \ \___  \  \ \___  \  
 \ \_____\  \ \_\ \_\  \ \_____\  \/\_____\  \/\_____\ 
  \/_____/   \/_/\/_/   \/_____/   \/_____/   \/_____/ 
        </pre>
        <div class="game">
            <span style="color: yellow;">Random Player (White)</span>
            <div class="board">
<pre class="board">
♜ ♞ ♝ ♛ ♚ ♝ ♞ ♜
♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
♙ ♙ ♙ ♙ ♙ ♙ ♙ ♙
♖ ♘ ♗ ♕ ♔ ♗ ♘ ♖</pre>
                <div class="game-over">
                    <p>GAME OVER</p>
                    <span>
                        - Outcome: Draw<br>
                        - Max moves reached: 200<br>
                        - Material White: 16<br>
                        - Material Black: 18
                    </span>
                </div>
            </div>
            <span style="color: yellow;">GPT-4o Mini (Black)</span>
        </div>
        <p class="intro">
            Can a Large Language Model play chess? Prompt it to move based on the board state, 
            hint it with legal moves, and it can make moves (though some struggle with instruction following) 
            and even explain its tactics or strategy.
            <br><br>
            But can LLMs make meaningful moves and win? 
            Let’s test them against a random player (a bot that picks legal moves randomly). 
            These Foundational Models, with their vast knowledge and reasoning abilities,
            should easily defeat a chaos monkey, right?
            <br><br>
            Let's find out ツ
        </p>
        <div class="button-container">
            <button class="selected" onclick="showPane('leaderboard')">Leaderboard</button>
            <button onclick="showPane('how-it-works')">How it works</button>
            <button onclick="showPane('considerations')">Notes</button>
        </div>
        <div id="leaderboard" class="pane">
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th title="Model playing as black against a Random Player">Player&nbsp;&nbsp;</th>
                            <th title="How often the player scored a win (due to check mate or the opponent failing to make)">Wins&nbsp;&nbsp;</th>
                            <th title="Percentage of games without a winner (e.g. reaching maximum number of 200 moves OR stalemate). Displays LLMs proficiency in chess.">Draws&nbsp;&nbsp;</th>
                            <th title="Number of LLM erroneous replies per 1000 moves - how often did an LLM fail to follow the instructions and make a move, e.g. due haluscinations picking illegal move and not conforming to communication protocol. Shows model's instruction following capabilities and halusinations resistance">Mistakes&nbsp;&nbsp;</th>
                            <th title="Number of token's generated per one move. Demonstrates model's verbosity.">Tokens&nbsp;&nbsp;</th>
                        </tr>
                    </thead>
                    <tbody>
                        <!-- Data will be populated here -->
                    </tbody>
                </table>

            </div>
            <p class="descriptions">
                <strong>METRICS:</strong><br><br>
                <strong>- Player:</strong> Model playing as black against a Random Player.<br>
                <strong>- Wins:</strong> How often the player scored a win (due to checkmate or the opponent failing to make a move). Displays LLMs' proficiency in chess.<br>
                <strong>- Draws:</strong> Percentage of games without a winner (e.g., reaching the maximum number of 200 moves or stalemate). Displays weaks' LLMs' proficiency in chess if it can't win.<br>
                <strong>- Mistakes:</strong> Number of LLM erroneous replies per 1000 moves - how often did an LLM fail to follow the instructions 
                and make a move. E.g., due to hallucinations, picking illegal moves, not conforming to the communication protocol. Shows the model's instruction-following capabilities and hallucination resistance.<br>
                <strong>- Tokens:</strong> Number of tokens generated per move. Demonstrates the model's verbosity.<br><br>
                <strong>NOTES:</strong><br><br>
                - LLMs played as black against a Random Player (as white).<br>
                - 30+ game simulations for Random Player vs. LLM.<br>
                - Bottom rows in green demonstrate how a Chess Engine (Stockfish v17) fares against a Random Player.<br>
                - 1000 simulations for Random Player vs. Chess Engine and Random vs. Random.<br>
                - <s>You see it right, LLMs scored 0 wins.</s> No longer the case, o1-mini being the 1st LLM scroging wins<br>
                - <s>Using Draws instead of Wins to evaluate LLMs' chess proficiency</s>.<br>
                - The default soritng is by Wins DESC, Draws DESC and Mistakes ASC <br>
                - Strong models (those ones winning) are judged (in Chess proficiency) by % Won, weak ones - by % Draws <br>
                - <strong>Mistakes</strong> metric gives an evaluation of LLMs' instruction-following capabilities and resistance to hallucinations (making up non-legal moves while having a list of legal moves provided in the prompt).<br>
                - Sort by <strong>Mistakes</strong> column and get a ranking by instruction-following ability (models with the least mistakes being better) <br>
            </p>
        </div>
        <div id="how-it-works" class="pane" style="display: none;">
            <p class="descriptions">
                <strong>Libraries and Dependencies Used:</strong><br><br>
                - <strong>chess:</strong> A Python library for handling chess game rules and basic operations, including board representation, legal move evaluation, and game state evaluation. This is not a chess engine runnig the actual caclulation of the best move.<br>
                - <strong>Microsoft Autogen</strong> is used as a backbone for LLM communication. It also implements the interaction between a Chess Board and custom agents like GameAgent, RandomPlayerAgent, AutoReplyAgent, and others for simulating different player types.<br>
                - <strong>Stockfish</strong> - the chess engine doing the actual best move calculation, used as a referrence to demonstrate what a real chess player's peformance is.<br>
                <br>
                <strong>Workflow:</strong><br><br>
                1. The game is initialized with a chess board and two players: a Random Player (as white) and an LLM (as black).<br>
                2. The game loop runs until a termination condition is met, such as checkmate, stalemate, or reaching the maximum number of moves.<br>
                3. Each player takes turns making a move. The Random Player selects a move randomly from the list of legal moves.<br>
                4. The LLM is prompted to make a move using a structured dialog, which includes actions like getting the current board state, retrieving legal moves, and making a move.<br>
                5. The game state is updated after each move, and the board is visualized if enabled.<br>
                6. Game statistics are generated and stored at the end of the game.<br>
                <br>
                <strong>Dialog:</strong><br><br>
                Here is an example of a dialog that prompts a model to make a move:<br>
            </p>
            <pre class="snippet">
<span style="color:green">Proxy (to Player_Black):</span>
You are a professional chess player and you play as black. Now is your turn to make a move. Before making a move you can pick one of the following actions:
- 'get_current_board' to get the schema and current status of the board
- 'get_legal_moves' to get a UCI formatted list of available moves
- 'make_move &lt;UCI formatted move&gt;' when you are ready to complete your turn (e.g., 'make_move e2e4')
Respond with the action.

<span style="color:green">Player_Black (to Proxy):</span>
get_current_board

<span style="color:green">Proxy (to Player_Black):</span>
<span style="font-family: Chess;">♞ ♝ ♛ ♚ ♝ ♞ ♜ ♜
♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
· · · · · · · ·
♙ ♙ ♙ ♙ ♙ ♙ ♙ ♙
♖ ♘ ♗ ♕ ♔ ♗ ♘ ♖
</span>

<span style="color:green">Player_Black (to Proxy):</span>
get_legal_moves

<span style="color:green">Proxy (to Player_Black):</span>
a7a6, b7b6, c7c6, d7d6, e7e6, f7f6, g7g6, h7h6, a7a5, b7b5, c7c5, d7d5, e7e5, f7f5, g7g5, h7h5

<span style="color:green">Player_Black (to Proxy):</span>
make_move e7e5

<span style="color:green">Proxy (to Player_Black):</span>
Move made, switching player
            </pre>
            <p class="descriptions">
                1 move - 1 dialog. The dialog is limmited to 10 turns (10 pairs of Proxy/Agent Request/Response messages). A maximum of 3 mistakes (not conforming to 
                communication notation, picking wrong action or wrong move) are allowed per dialog. 
                The game is terminanted and LLM is given a loss if the max turns OR of number of LLM mistakes is reached in the given dialog.
            </p>
        </div>
        <div id="considerations" class="pane" style="display: none;">
            <p class="descriptions">
                <span style="color:white;">
                -------------------<br>
                January 24, 2025: x.ai's Grok<br>
                -------------------<br><br></span>

                - Added <strong>Grok-2</strong> results (<strong>grok-2-1212</strong>), and it scored 2 wins in 32 games.<br>
                &nbsp;&nbsp; - Latter I've added 17 more runs, no more wins, win rate and material diff went a bit down.<br><br>

                - Originally, I adhered to exactly 30 game runs per model. Recently, I have been conducting more runs
                where possible (and financially reasonable—some models can be extremely expensive).<br><br>

                - More runs to some models might provide them with an advantage.
                For instance, if a model is weak in chess and can barely score a single win, 
                additional runs increase the chances of an occasional win, giving this model
                an automatic edge over other models (potentially better all-around models)
                that consistently achieve draws.<br><br>

                - <strong>grok-2-1212</strong>, with a 6% win rate, places the model at the top of the list among
                other non-reasoning models (<strong>GPT-4o</strong>, <strong>Claude 3.5 Sonnet</strong>), yet it has a low draw rate.
                The reason for this is that <strong>Grok-2</strong> sometimes fails to follow instructions, 
                neglecting to pick the correct move from the list of legal ones (which it had responsibly requested).
                Could this be due to the tokenizer or some "SolidGoldMagikarp" out-of-distribution token
                combination?<br><br>
                
                - Despite this, <strong>Grok-2</strong> is an intriguing non-reasoning model. It maintained a positive 
                material count throughout the games (with a material difference of 2.63).<br><br>

                - Example of <strong>Grok-2</strong> failing to pick a legal move and hallucinating one not on the list &lt;
            </p>
            <div class="expandable-snippet">
                <button class="toggle-snippet" onclick="toggleSnippet(this)">Show Snippet</button>
                <pre class="snippet" style="display: none;">
<span style="color:green">Player_Black (to Proxy):</span>

Thank you for providing the current board status. Now, I'll check the legal moves available to me. Please provide the result of 'get_legal_moves'.

--------------------------------------------------------------------------------
<span style="color:green">Proxy (to Player_Black):</span>

h8g8,f8e7,f8d6,f8c5,f8b4,f8a3,e8d8,e8e7,e8d7,a8d8,a8c8,a8b8,e5e7,e5f6,e5e6,e5d6,e5g5,e5f5,e5d5,e5c5,e5b5,e5a5,e5f4,e5e4,e5d4,e5g3,e5c3,e5h2,e5b2,e5a1,h7g6,h7h6,c7c6,b7b6,a7a6,h7h5,c7c5,b7b5,a7a5

--------------------------------------------------------------------------------
<span style="color:green">Player_Black (to Proxy):</span>

After reviewing the current board and the list of legal moves, I will make my move. I choose to move the knight from e5 to f3, attacking the king and potentially setting up a fork. Please execute 'make_move e5f3'.

--------------------------------------------------------------------------------
<span style="color:green">Proxy (to Player_Black):</span>

Failed to make move: illegal uci: 'e5f3' in r3kb1r/ppp3pp/6P1/4q1N1/P3N3/1P1p3P/R1pP1P2/2B2K1R b - - 0 26

--------------------------------------------------------------------------------
<span style="color:green">Player_Black (to Proxy):</span>

Apologies for the mistake. Let me correct my move. I will move the knight from e5 to d3, which is a legal move and aims to challenge the opponent's control of the center. Please execute 'make_move e5d3'.

--------------------------------------------------------------------------------
<span style="color:green">Proxy (to Player_Black):</span>

Failed to make move: illegal uci: 'e5d3' in r3kb1r/ppp3pp/6P1/4q1N1/P3N3/1P1p3P/R1pP1P2/2B2K1R b - - 0 26

--------------------------------------------------------------------------------
<span style="color:green">Player_Black (to Proxy):</span>

I apologize for the confusion. Let me review the legal moves again and choose a different move. I will move the queen from e5 to e2, which is a legal move and aims to put pressure on the opponent's position. Please execute 'make_move e5e2'.

--------------------------------------------------------------------------------
<span style="color:green">BREAKING &gt;&gt;&gt; Failed to make move: illegal uci: 'e5e2' in r3kb1r/ppp3pp/6P1/4q1N1/P3N3/1P1p3P/R1pP1P2/2B2K1R b - - 0 26</span>
<span style="color:green">Proxy (to Player_Black):</span>

Too many wrong actions
                    </pre>
            </div>
            <p class="descriptions">
                <span style="color:white;">
                -------------------<br>
                January 22, 2025: R1 Challenges o1, thinking "surrogates"<br>
                -------------------<br><br></span>
        
                Released on January 21, <code>DeepSeek-R1</code> made a significant impact by showing similar
                or even better performance than OpenAI's <code>o1</code> models across a range of math and
                coding benchmarks. All of this comes at a fraction of the cost when accessing
                their 671B model through their API. Besides, the model weights are open, and
                there are plenty of details about the training process—a stark contrast to
                OpenAI's secrecy regarding the <code>o1/o3</code> family of models. And that's not all! For
                those interested in local models, there are smaller "distilled" reasoning models
                at 70B, 32B, 14B, etc., sizes.<br><br>

                So far, none of the so-called reasoning or thinking models have come close to <code>o1</code>
                (<code>qwq-32b-preview</code>, <code>sky-t1-32b-preview</code>, <code>gemini-2.0-flash-thinking-exp-1219</code>). They
                were a step back in instruction following and could hardly be steered when
                compared to non-thinking models. Let alone, they didn't perform as well in chess.
                I call them surrogates, verbosity generators mimicking but not replicating <code>o1</code>'s
                behavior.<br><br>

                Deepseek-R1 (<code>deepseek-reasoner-r1</code>) was an extremely curious model to test, and
                now there are evaluation results in the Leaderboard.<br><br>

                Additionally, Google has just released an update to its Gemini 2.0 Thinking Model
                (<code>gemini-2.0-flash-thinking-exp-01-21</code>), which previously showed poor results. Two
                very interesting releases in two days...<br><br>

                <strong>NOTES:</strong><br><br>

                - There are now three models in a league of their own: <code>o1-mini</code>, <code>o1-preview</code>, and now
                <code>deepseek-reasoner-r1</code>. These are the only models with significant wins (&gt;10%) and
                maintaining a positive material difference (~9) as opposed to a negative material
                difference for the resetGameInterval.<br><br>

                - <code>deepseek-reasoner-r1</code> is much more verbose (almost twice as many tokens as <code>o1</code>) and
                also makes more mistakes. Many games were lost due to the model making mistakes,
                unlike <code>o1</code> models, which mostly fell out of the games due to losing or drawing
                (e.g., stalemate, insufficient material).<br><br>

                - <code>deepseek-reasoner-r1</code> showed distinctive behavior: sometimes it made moves without
                requesting the board state or legal moves list (i.e., issues make_move action in
                the very first response), sometimes it only requested the board state without
                getting the list of moves and decided on the move without a hint (in some cases
                picking the right move, in others wrong).<br><br>

                - Overall, <code>o1</code> models are better all-round models (compared to R1), with better chess
                performance, as well as instruction following (Mistakes) and Durability (not
                falling out of the games, doing more moves on average).<br><br>

                - The rest of the "thinking" models (<code>sky-t1-32b-preview@q4_0</code>, <code>qwq-32b-preview@q4_k_m</code>,
                <code>deepseek-r1-distill-qwen-32b@q4_k_m</code>, <code>deepseek-r1-distill-qwen-14b@q8_0</code>,
                <code>gemini-2.0-flash-thinking-exp-1219</code>) can hardly follow prompt instructions, lasting
                on average 2~14 moves.<br><br>

                - Distilled dense 14B and 32B versions of <code>Deepseek-R1</code> (<code>deepseek-r1-distill-qwen-32b@q4_k_m</code>,
                <code>deepseek-r1-distill-qwen-14b@q8_0</code>) did not impress—very poor instruction following,
                not even close to the full 671B MoE model. Could quantization have an effect?
                Don't know.<br><br>

                - Distilled R1 models use <code>&lt;think&gt;&lt;/think&gt;</code> tags to enclose
                "reasoning" tokens (those ones hidden for <code>o1</code>), sometimes they fail to put the
                opening <code>&lt;think&gt;</code> tag (most often in the very first reply).<br><br>

                - The new Gemini Thinking model (<code>gemini-2.0-flash-thinking-exp-01-21</code>) did improve a
                lot in instruction following. While the previous December version
                (<code>gemini-2.0-flash-thinking-exp-1219</code>) was almost useless, staying in the game on
                average for 2.3 moves (i.e., breaking instruction prompts in the 2nd move it was
                asked to make), the new model lasted on average 40 moves and showed far fewer
                mistakes.<br><br>

                In terms of chess, the newer <code>gemini-2.0-flash-thinking-exp-01-21</code> showed a negative
                material difference and negligible draw count—way below other non-thinking models
                (even Google's own Gemma 2 9B).<br><br>

                - <code>gemini-2.0-flash-thinking-exp-01-21</code> also demonstrated much less verbosity in the
                outputs, providing to-the-point answers. Didn't investigate it deeply, yet it
                seems that now Gemini Thinking does proper isolation of reasoning tokens. Per
                <a href="https://ai.google.dev/gemini-api/docs/thinking">docs</a>, "On a standard
                request, the model responds with two parts, the thoughts and the model response.
                You can check programmatically if a part is a thought or not by seeing if the
                part.thought field is set to True." Autogen doesn't seem to register thought
                tokens in completion stats for Flash Thinking while it does so for R1.<br><br>

                - Most of the failures/losses from <code>gemini-2.0-flash-thinking-exp-01-21</code> 
                were due to server errors. Occasionally, the server returned an empty message or flagged 
                the request with a "RECITATION" error, among others. 
                Clearly, there's some instability in generation on the server side.<br><br>

                - Earlier non-<code>o1</code> thinking models did not do well with splitting the reasoning/thinking part
                from the final answer, R1 and newer Gemini Thinking do that (yet it didn't help
                the distilled R1 models).<br><br>

                <span style="color:white;">
                -------------------<br>
                January 21, 2025: More Models, More Tests<br>
                -------------------<br><br></span>
                - Added the new Claude 3.5 Sonnet (<code>claude-v3-5-sonnet-v2</code>) and also retested the previous version (<code>claude-v3-5-sonnet-v1</code>), giving them more runs
                (60 this time). What surprised me was not just the fact that <code>v2</code> showed some wins, but that <code>v1</code> demonstrated even more wins 
                (although losing to the random player more often). 
                Back in November, when I did 30 runs of Sonnet 3.5, there were 0 wins. 
                Did Anthropic change something in <code>v1</code> as well, or was it just a question of stochasticity?
                <br><br>
                - Conducted more runs of GPT-4o models (<code>gpt-4o-2024-05-13</code>, <code>gpt-4o-2024-08-06</code>, <code>gpt-4o-2024-11-20</code>). The August version also achieved a win, yet
                it also had its first non-zero mistake (previous 30-run result had a perfect 0 mistakes) - this time, that happened due to Azure's content
                filter halting one of the games.
                <br><br>
                - OpenAI's and Anthropic's models are still the most durable models, i.e.,
                demonstrating the least number of mistakes, staying in almost all games
                until the very end without falling off due to a random glitch/hallucination
                or breaking the prompt instructions. Clearly, it is a challenge for most models
                to sustain a 100-turn (200-move game) with an average of 3-4 dialog interactions per game turn. 
                <br><br>
                - Deepseeks R1 distilled into 32B Qwen (<code>deepseek-r1-distill-qwen-32b@q4_k_m</code>) didn't perform well
                - barely able to move past a couple of turns in the chess game, no better than <code>QwQ</code> or Flash Thinking.
                <br><br>
                - <code>o1</code> models are still special being the only models maintaing positive material diff (gaining more piaces rather than losing them)
                versus most other models having a significant negative diff suggesting the wins for non-<code>o1</code> models can hardly be attributed
                to models reasoning performance (i.e. with the models meaningfully moving the pieces rather than faking a game).
                <br><br>
                <span style="color:white;">
                -------------------<br>
                January 16, 2025: First wins by non-reasoning model<br>
                -------------------<br><br></span>
                The November 2024 update to <code>GPT-4o</code> demonstrated a few wins, something non-o1 models couldn't do before. Yet, what is curious is that the updated
                model not only scored wins but also lost to the Random Player more often than others.
                <br><br>
                <span style="color:white;">
                -------------------<br>
                January 14, 2025 : Reasoning Models, First Wins<br>
                -------------------<br><br></span>
                In my initial notes (see below), I expressed skepticism about LLMs ever being capable of achieving meaningful wins against a Random Player 
                (let alone a dedicated chess engine). I also mentioned the ARC Challenge as a similar benchmark that is difficult or unsolvable by LLMs.
                Yet, in late December 2024, OpenAI introduced their o3 model, which excelled in 
                <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">ARC-AGI-1</a>.<br><br>
                Around the same time, I received API access to o1-mini, and it was impressive! o1-mini scored 9 wins in 30 games (30%), 
                making it the first model
                capable of meaningful chess games. Next, I tried o1-preview, achieving an even more impressive 46.7% win rate.<br><br>
                The o1 models did something different; they played a meaningful game rather than randomly moving pieces on the board as 
                seen with older "non-reasoning" models. They formed a league of their own—a "strong" model capable of winning.<br><br>
                I also tested a number of other reasoning models introduced after o1—Google's Gemini 2.0 Flash Thinking (via API), 
                QwQ 32B (4-bit quantized via LM Studio), and Sky T1 32B (also 4-bit quantized via LM Studio).
                The results were underwhelming, to say the least—those 3 models couldn't complete even a single game, often failing 
                after a few moves (due to breaking the communication protocol or making illegal moves).
                Just like the o1 models, they produced lots of tokens, yet unlike o1, 
                which seems to have 2 separate stages—thinking (those hidden tokens) and producing the 
                final answer—the other "thinking" models just stream lots of tokens without caring about 
                following any instructions as to what they are asked to produce.
                Verbosity and failing to adhere to instructions seem to plague not just the o1 competitors 
                but also newer models (e.g., check Deepseek v3).<br><br>
                <strong>NOTES:</strong><br><br>
                - When LLMs achieve a 90% win rate against a Random Player, I will replace it with Stockfish.<br>
                - You can find conversation logs for 
                o1-mini <a href="https://github.com/maxim-saplin/llm_chess/blob/main/_logs/no_reflection/_21.12.2024_o1-mini-2024-09-12/output.txt">here</a> 
                and for o1-preview <a href="https://github.com/maxim-saplin/llm_chess/blob/main/_logs/no_reflection/_03.01.2025_o1-preview-2024-09-12_2/output.txt">
                here</a><br><br>
                - Conversation logs for <a href="https://github.com/maxim-saplin/llm_chess/blob/main/_logs/no_reflection/_09.01.2025_gemini-2.0-flash-thinking-exp-1219/output.txt">Gemini 2.0 Thinking</a>,
                <a href="https://github.com/maxim-saplin/llm_chess/blob/main/_logs/no_reflection/_14.01.2025_sky-t1-32b-preview_q4-0/output.txt">QwQ</a>
                and <a href="https://github.com/maxim-saplin/llm_chess/blob/main/_logs/no_reflection/_14.01.2025_sky-t1-32b-preview_q4-0/output.txt">Sky T1</a><br><br>
                - While o1 models used a lot of tokens (1221 tokens per move for o1-mini and 2660 
                for o1-preview vs. below 100 tokens for an average model), their replies were concise (most of the tokens 
                were hidden ones) and had few mistakes.<br><br>
                - OpenAI models might have an advantage over other models because the prompts have been crafted with GPT-4 models.
                This might explain why many models perform worse. Contrary to this point, there are non-OpenAI models (e.g., Anthropic)
                that do not struggle with instruction following. I believe the factor of prompts and adjusting prompts for each specific model plays a minor role.<br><br>
                - An idea: while other (non-o1) reasoning models might be poor instruction followers, why 
                not introduce a deliberate step of final answer synthesizing? I.e., use a cheaper LLM (e.g., 4o-mini) 
                to get the long scroll from a reasoning model and prompt it to produce a final answer. 
                Will the reasoning models show better chess abilities?
                <br><br>
                <span style="color:white;">
                -------------------<br>
                November 12, 2024 : Initial Observations, Opinion<br>
                -------------------<br><br></span>
                Language Models can make moves in a chess game, but they cannot actually play chess or progress towards a victory. 
                The experiment started after taking a class at DeepLearning, which taught the Microsoft Autogen framework. One of the classes demonstrated
                a simulation of a chess game between LLMs. I was immediately intrigued by the idea of putting different LLMs head-to-head in a chess game competition. 
                However, I was surprised that the naive prompting strategy from the class never led to a game completion. Extending prompts didn't help. 
                I ended up testing LLMs' performance in a chess game using a Random Player. A human player with reasonable chess skills would have no problems winning 
                against a random player, yet LLMs failed miserably in this competition.
                <br><br>
                I suspect that crushing this "LLM Chess" eval might be as hard as the <a href="https://arcprize.org">ARC Challenge</a>—a benchmark created to demonstrate 
                the true nature of text-generating LLMs, exploit their weaknesses, and show how LLMs struggle with reasoning. "It's easy for humans, but hard for AI."
                <br><br>
                LLMs and Transformer-based models can be trained specifically to play chess. There are projects on the internet where people have fine-tuned LLMs
                as chess players. Yet, those are specialized models that can't be good chat models.
                <br><br>
                I am looking forward to testing new releases of SOTA and frontier models. It would be great to see a model that starts scoring wins against
                the chaos monkey while maintaining performance at traditional chat tasks.
                <br>
                <br>
                <strong>NOTES:</strong><br><br>
                - More data on game simulations is available <a href="https://github.com/maxim-saplin/llm_chess/blob/main/docs/_data/refined.csv">here</a> and 
                <a href="https://github.com/maxim-saplin/llm_chess/blob/main/docs/_data/aggregate_models.numbers">here</a>.<br><br>
                - No history of moves is available to LLM, no reflection used (giving the model "time to think").<br><br>
                &nbsp;- Experiments with reflection suggest that LLMs do even worse when they are prompted to evaluate options before making a move
                (<a href="https://github.com/maxim-saplin/llm_chess/blob/main/_logs/_not_OK_logs/reflection/aggregate_results.csv">reflection results</a>).
                &nbsp;- Could LLMs improve their performance if given the whole log of the game?<br><br>
                - The chess engine ( <a href="https://stockfishchess.org">Stockfish</a>) has a 100% win rate with an average game taking 32 moves to complete.<br><br>
                &nbsp;- Stockfish 17, macOS, 0.1ms time limit (vs 0.1s default) - with decreased performance, Stockfish dominates over Random Player.<br><br>
                - Random Player (as white) wins over Random Player (as black) in 10.5% of cases - LLMs scored 0 wins.<br><br>
                &nbsp;- Indeed, giving the right of the first move gives an advantage to the white player.<br><br>
                &nbsp;- LLMs do worse than random players.<br><br>
                &nbsp;- It's as if an LLM had no goal to win, as if it was its intention to keep the game going. What if I prompted it and told it that 200 moves is the maximum and the game ends after? Would it try harder? Can adding to a system prompt an explicitly instruction to Win help?<br><br>
                - While some models are less verbose and follow the rules strictly (e.g., OpenAI), others are verbose.<br><br>
                &nbsp;- Initially, I used exact match when communicating with an LLM and prompted it to reply with action names (show board, get legal moves, make move) - worked well with OpenAI.<br><br>
                &nbsp;- After the list of models was extended, the original prompts had issues steering them.<br><br>
                &nbsp;- As a result, I changed the communication protocol to use regex and be tolerant to reply format, doing its best to extract action and arguments from LLM replies.<br><br>
                - Since models don't score any Wins there must be some alternative metric demosntrating game progress.<br><br>
                &nbsp;- For the time being using Draws, the more draws - the better.<br><br>
                &nbsp;- Yet most of the draws scored are due to hitting the 200 max moves limmit and hence the metric demonstrates the adherence to communication protocol/prompt cpnverntions.<br><br>
                &nbsp;- Logs also contain "Material Count" metrics - the weighted scores of piaces a player has, at the beginning a player has a total of 39 units of material.<br><br>
                &nbsp;- Material difference could be a good metric to evluate progress, the player having more material left as the game progress is at a better position.<br><br>
                &nbsp;- Yet most of the models demonstrated negative material difference and one of the models (gpt-35-turbo-0125) failed to make a single move having a material difference at 0 putting it above models that had negative material while staying in the game much longer.<br><br>
                &nbsp;- It might be resonable to create a computed metric that account for both the material and length of the game, addressing the endless pointless game concern, as well as never changing material diff due to failing to early.<br><br>
                - What if the model is not given the list of legal moves? Will the model figuring out legal moves on their own and struggle to progress? Can giving the models a list of legal moves essentially break reasoning turning the game into simple instruction following (i.e. pick one item from the list rather than win in the game)?<br><br>
                - The older GPT-4 Turbo did better than newer GPT-4o version, this is a yet another eval demonstrating how newer models performed worse supporting the claim the the 4o family of models are smaller and cheaper to run models.<br><br>
            </p>
        </div>
        <a class="gh" href="https://github.com/maxim-saplin/llm_chess/" target="_blank">
            <img src="icons/gh.png" alt="GitHub Icon">
            Project's GitHub
        </a>
    </div>
    <div id="popup" style="display: none;">
        <p id="total-games"></p>
        <p id="wins"></p>
        <p id="losses"></p>
        <p id="wins_minus_losses"></p>
        <p id="draws"></p> 
        <p id="average-moves"></p>
        <p id="material-diff"></p>
    </div>
</body>
</html>
