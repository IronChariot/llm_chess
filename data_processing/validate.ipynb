{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas Jinja2 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from aggregate_logs_to_csv import aggregate_models_to_csv, MODEL_OVERRIDES\n",
    "from aggr_logs_to_plain_csv import aggregate_logs_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Aggregate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = \"_logs/no_reflection\"\n",
    "AGGREGATE_CSV = os.path.join(LOGS_DIR, \"aggregate_models.csv\")\n",
    "REFINED_CSV = \"data_processing/refined.csv\"\n",
    "\n",
    "aggregate_models_to_csv(\"../_logs/no_reflection\",\"aggr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"aggr.csv\"\n",
    "# CSV cols:\n",
    "# model_name,total_games,black_llm_wins,white_rand_wins,draws,black_llm_wins_percent,black_llm_draws_percent,llm_total_moves,llm_wrong_actions,llm_wrong_moves,llm_avg_material,llm_std_dev_material,rand_avg_material,rand_std_dev_material,material_diff_llm_minus_rand,material_diff_llm_minus_rand_per_100moves,wrong_actions_per_100moves,wrong_moves_per_100moves,wrong_actions_per_1000moves,wrong_moves_per_1000moves,mistakes_per_1000moves,std_dev_wrong_actions_per_1000moves,std_dev_wrong_moves_per_1000moves,std_dev_mistakes_per_1000moves,average_moves,std_dev_moves,completion_tokens_black,completion_tokens_black_per_move,min_moves,max_moves,prompt_tokens_black,total_tokens_black,moe_material_diff,moe_avg_moves,moe_wrong_actions_per_1000moves,moe_wrong_moves_per_1000moves,moe_mistakes_per_1000moves\n",
    "\n",
    "df_aggr = pd.read_csv(csv_file_path)\n",
    "print(df_aggr.to_string(index=False))\n",
    "\n",
    "# selected_columns = df_aggr[[\"model_name\", \"total_games\", \"wrong_actions_per_100moves\", \"wrong_moves_per_100moves\", \"min_moves\", \"max_moves\", \"average_moves\", \"std_dev_moves\"]]\n",
    "\n",
    "# Print the DataFrame as a properly tabbed table with headers\n",
    "# print(selected_columns.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Flattened (Plain) CSV with Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logs_to_dataframe(logs_path, output_csv, model_dict):\n",
    "    \"\"\"\n",
    "    Process logs into a DataFrame, substitute model names, and return the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        logs_path (str): Path to the logs directory.\n",
    "        output_csv (str): Path to save the intermediate CSV file.\n",
    "        model_dict (dict): Dictionary for substituting model names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with substituted model names.\n",
    "    \"\"\"\n",
    "    # Aggregate logs into a CSV\n",
    "    aggregate_logs_to_csv(logs_path, output_csv)\n",
    "\n",
    "    # Read the aggregated CSV into a DataFrame\n",
    "    df_plain = pd.read_csv(output_csv)\n",
    "\n",
    "    # Insert the 'model' column based on 'player_black_model'\n",
    "    df_plain.insert(df_plain.columns.get_loc(\"path\") + 1, \"model\", df_plain[\"player_black_model\"])\n",
    "\n",
    "    # Replace model names in the DataFrame using model_dict values\n",
    "    def substitute_model_names(df, model_dict):\n",
    "        def get_correct_model_name(row):\n",
    "            key = next((k for k in model_dict if os.path.dirname(row.path).endswith(k)), None)\n",
    "            return model_dict[key] if key else row[\"model\"]  # Default to the original model if no match is found\n",
    "\n",
    "        df[\"model\"] = df.apply(get_correct_model_name, axis=1)\n",
    "\n",
    "    # Apply the substitution logic\n",
    "    substitute_model_names(df_plain, model_dict)\n",
    "\n",
    "    return df_plain\n",
    "\n",
    "# Example usage\n",
    "df_plain = process_logs_to_dataframe(\"../_logs/no_reflection\", \"plain.csv\", MODEL_OVERRIDES)\n",
    "display(df_plain.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Aggr to Aggr-from-Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare aggregates from aggr.csv to thoses ones obtained from plain.csv, check number of games/logs to to match (the number of log files == sum of total games)\n",
    "\n",
    "# df_plain columns\n",
    "# path,time_started,winner,reason,number_of_moves,player_white_name,player_white_wrong_moves,player_white_wrong_actions,player_white_reflections_used,player_white_reflections_used_before_board,player_white_model,material_count_white,material_count_black,player_black_name,player_black_wrong_moves,player_black_wrong_actions,player_black_reflections_used,player_black_reflections_used_before_board,player_black_model,black_model_prompt_tokens,black_model_completion_tokens,black_model_total_tokens\n",
    "\n",
    "\n",
    "# Group the data by 'player_black_model' and calculate the number of moves for each model\n",
    "grouped_data = df_plain.groupby('model')['number_of_moves']\n",
    "\n",
    "aggregates_from_plain = df_plain.groupby('model').agg(\n",
    "    total_games=('number_of_moves', 'count'),\n",
    "    black_llm_wins=('winner', lambda x: (x == 'Player_Black').sum()),\n",
    "    white_rand_wins=('winner', lambda x: (x == 'Random_Player').sum()),\n",
    "    draws=('winner', lambda x: (x == 'NONE').sum()),\n",
    "    black_llm_wins_percent=('winner', lambda x: (x == 'Player_Black').sum() / len(x) * 100),\n",
    "    black_llm_draws_percent=('winner', lambda x: (x == 'NONE').sum() / len(x) * 100),\n",
    "    sum_wrong_actions=('player_black_wrong_actions', 'sum'),\n",
    "    sum_wrong_moves=('player_black_wrong_moves', 'sum'),\n",
    "    sum_moves=('number_of_moves', 'sum'),\n",
    "    min_moves=('number_of_moves', 'min'),\n",
    "    max_moves=('number_of_moves', 'max'),\n",
    "    average_moves=('number_of_moves', 'mean'),\n",
    "    std_dev_moves=('number_of_moves', lambda x: x.std(ddof=1)),  # Sample standard deviation\n",
    "    average_material_count_white=('material_count_white', 'mean'),\n",
    "    std_dev_material_count_white=('material_count_white', lambda x: x.std(ddof=1)),\n",
    "    std_err_material_count_white=('material_count_white', lambda x: x.std(ddof=1) / (len(x) ** 0.5)),\n",
    "    average_material_count_black=('material_count_black', 'mean'),\n",
    "    std_dev_material_count_black=('material_count_black', lambda x: x.std(ddof=1)),\n",
    "    std_err_material_count_black=('material_count_black', lambda x: x.std(ddof=1) / (len(x) ** 0.5)),\n",
    "    black_model_prompt_tokens=('black_model_prompt_tokens', 'sum'),\n",
    "    average_black_model_prompt_tokens=('black_model_prompt_tokens', 'mean'),\n",
    "    std_dev_black_model_prompt_tokens=('black_model_prompt_tokens', lambda x: x.std(ddof=1)),\n",
    "    std_err_black_model_prompt_tokens=('black_model_prompt_tokens', lambda x: x.std(ddof=1) / (len(x) ** 0.5)),\n",
    "    black_model_completion_tokens=('black_model_completion_tokens', 'sum'),\n",
    "    average_black_model_completion_tokens=('black_model_completion_tokens', 'mean'),\n",
    "    std_dev_black_model_completion_tokens=('black_model_completion_tokens', lambda x: x.std(ddof=1)),\n",
    "    std_err_black_model_completion_tokens=('black_model_completion_tokens', lambda x: x.std(ddof=1) / (len(x) ** 0.5)),\n",
    "    black_model_total_tokens=('black_model_total_tokens', 'sum'),\n",
    "    average_black_model_total_tokens=('black_model_total_tokens', 'mean'),\n",
    "    std_dev_black_model_total_tokens=('black_model_total_tokens', lambda x: x.std(ddof=1)),\n",
    "    std_err_black_model_total_tokens=('black_model_total_tokens', lambda x: x.std(ddof=1) / (len(x) ** 0.5))\n",
    ").reset_index() \n",
    "\n",
    "# # Now compute normalized values\n",
    "# aggregates['wrong_actions_per_100moves'] = (aggregates['sum_wrong_actions'] / aggregates['sum_moves']) * 100\n",
    "# aggregates['wrong_moves_per_100moves'] = (aggregates['sum_wrong_moves'] / aggregates['sum_moves']) * 100\n",
    "\n",
    "# Calculate and print the sum of total_games in df_aggr\n",
    "df_aggr_total_games_sum = df_aggr[\"total_games\"].sum()\n",
    "print(f\"Sum of total_games in df_aggr: {df_aggr_total_games_sum}\")\n",
    "\n",
    "# Calculate and print the sum of total_games in aggregates\n",
    "aggregates_total_games_sum = aggregates_from_plain[\"total_games\"].sum()\n",
    "print(f\"Sum of total_games in aggregates: {aggregates_total_games_sum}\")\n",
    "\n",
    "# Print column names from df_aggr\n",
    "print(\"Columns in df_aggr:\")\n",
    "print(df_aggr.columns.tolist())\n",
    "\n",
    "# Print column names from aggregates\n",
    "print(\"Columns in aggregates_from_plain:\")\n",
    "print(aggregates_from_plain.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the calculated aggregates\n",
    "print(aggregates_from_plain.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the best matches between df_aggr and aggregates_from_plain column names\n",
    "column_mapping = {\n",
    "    'model_name': 'model',\n",
    "    'total_games': 'total_games',\n",
    "    'black_llm_wins': 'black_llm_wins',\n",
    "    'white_rand_wins': 'white_rand_wins',\n",
    "    'draws': 'draws',\n",
    "    'black_llm_wins_percent': 'black_llm_wins_percent',\n",
    "    'black_llm_draws_percent': 'black_llm_draws_percent',\n",
    "    'llm_total_moves': 'sum_moves',\n",
    "    'llm_wrong_actions': 'sum_wrong_actions',\n",
    "    'llm_wrong_moves': 'sum_wrong_moves',\n",
    "    'llm_avg_material': 'average_material_count_black',\n",
    "    'llm_std_dev_material': 'std_dev_material_count_black',\n",
    "    'rand_avg_material': 'average_material_count_white',\n",
    "    'rand_std_dev_material': 'std_dev_material_count_white',\n",
    "    'material_diff_llm_minus_rand': None,  # No direct match in aggregates_from_plain\n",
    "    'material_diff_llm_minus_rand_per_100moves': None,  # No direct match in aggregates_from_plain\n",
    "    'wrong_actions_per_100moves': None,  # No direct match in aggregates_from_plain\n",
    "    'wrong_moves_per_100moves': None,  # No direct match in aggregates_from_plain\n",
    "    'wrong_actions_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'wrong_moves_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'mistakes_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'std_dev_wrong_actions_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'std_dev_wrong_moves_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'std_dev_mistakes_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'average_moves': 'average_moves',\n",
    "    'std_dev_moves': 'std_dev_moves',\n",
    "    'completion_tokens_black': 'black_model_completion_tokens',\n",
    "    'completion_tokens_black_per_move': None,  # No direct match in aggregates_from_plain\n",
    "    'min_moves': 'min_moves',\n",
    "    'max_moves': 'max_moves',\n",
    "    'prompt_tokens_black': 'black_model_prompt_tokens',\n",
    "    'total_tokens_black': 'black_model_total_tokens',\n",
    "    'moe_material_diff': None,  # No direct match in aggregates_from_plain\n",
    "    'moe_avg_moves': None,  # No direct match in aggregates_from_plain\n",
    "    'moe_wrong_actions_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'moe_wrong_moves_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "    'moe_mistakes_per_1000moves': None,  # No direct match in aggregates_from_plain\n",
    "}\n",
    "\n",
    "# Iterate over the rows in df_aggr\n",
    "for index, row in df_aggr.iterrows():\n",
    "    model_name = row['model_name']\n",
    "    \n",
    "    # Find the corresponding row in aggregates_from_plain\n",
    "    matching_row = aggregates_from_plain[aggregates_from_plain['model'] == model_name]\n",
    "    \n",
    "    if matching_row.empty:\n",
    "        print(f\"Model '{model_name}' not found in aggregates_from_plain.\")\n",
    "        continue\n",
    "    \n",
    "    # Compare the values of mapped columns\n",
    "    for df_aggr_col, aggregates_col in column_mapping.items():\n",
    "        if aggregates_col is None:\n",
    "            # Skip columns that have no mapping\n",
    "            continue\n",
    "        \n",
    "        df_aggr_value = row[df_aggr_col]\n",
    "        \n",
    "        try:\n",
    "            # Safely access the value in matching_row\n",
    "            aggregates_value = matching_row.iloc[0].get(aggregates_col, None)\n",
    "        except KeyError:\n",
    "            print(f\"Column '{aggregates_col}' not found in aggregates_from_plain for model '{model_name}'.\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure both values are converted to numeric if possible\n",
    "        try:\n",
    "            df_aggr_value = pd.to_numeric(df_aggr_value, errors='coerce')\n",
    "            aggregates_value = pd.to_numeric(aggregates_value, errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting values to numeric for column '{df_aggr_col}': {e}\")\n",
    "            continue\n",
    "        \n",
    "        if not pd.isna(df_aggr_value) and not pd.isna(aggregates_value):\n",
    "            if not np.isclose(df_aggr_value, aggregates_value, atol=1e-6):\n",
    "                print(f\"Discrepancy for model '{model_name}' in column '{df_aggr_col}':\")\n",
    "                print(f\"  df_aggr value: {df_aggr_value}\")\n",
    "                print(f\"  aggregates_from_plain value: {aggregates_value}\")\n",
    "        elif pd.isna(df_aggr_value) != pd.isna(aggregates_value):\n",
    "            print(f\"Discrepancy for model '{model_name}' in column '{df_aggr_col}':\")\n",
    "            print(f\"  df_aggr value: {df_aggr_value}\")\n",
    "            print(f\"  aggregates_from_plain value: {aggregates_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows in df_aggr for the specified model\n",
    "df_aggr_filtered = df_aggr[df_aggr['model_name'] == 'gemini-2.0-flash-thinking-exp-01-21']\n",
    "\n",
    "# Filter the rows in aggregates_from_plain for the specified model\n",
    "aggregates_filtered = aggregates_from_plain[aggregates_from_plain['model'] == 'gemini-2.0-flash-thinking-exp-01-21']\n",
    "\n",
    "# Print the first 2 rows from each dataset\n",
    "print(\"Rows from df_aggr:\")\n",
    "print(df_aggr_filtered.head(1).to_string())\n",
    "\n",
    "print(\"\\nRows from aggregates_from_plain:\")\n",
    "print(aggregates_filtered.head(1).to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
